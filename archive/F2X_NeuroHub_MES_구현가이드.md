# F2X NeuroHub MES êµ¬í˜„ ê°€ì´ë“œ

**ê°œë°œìë¥¼ ìœ„í•œ ê¸°ìˆ  ì°¸ì¡° ë¬¸ì„œ**

**Version:** 1.0
**ì‘ì„±ì¼:** 2025.11.10
**ëŒ€ìƒ:** Backend/Frontend ê°œë°œì, DevOps ì—”ì§€ë‹ˆì–´

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œë°œ í™˜ê²½ ì„¤ì •](#1-ê°œë°œ-í™˜ê²½-ì„¤ì •)
2. [ë°ì´í„°ë² ì´ìŠ¤ êµ¬í˜„](#2-ë°ì´í„°ë² ì´ìŠ¤-êµ¬í˜„)
3. [Backend API êµ¬í˜„](#3-backend-api-êµ¬í˜„)
4. [Frontend êµ¬í˜„](#4-frontend-êµ¬í˜„)
5. [ë³´ì•ˆ êµ¬í˜„](#5-ë³´ì•ˆ-êµ¬í˜„)
6. [í…ŒìŠ¤íŠ¸ êµ¬í˜„](#6-í…ŒìŠ¤íŠ¸-êµ¬í˜„)
7. [ë°°í¬ ë° ìš´ì˜](#7-ë°°í¬-ë°-ìš´ì˜)

---

## 1. ê°œë°œ í™˜ê²½ ì„¤ì •

### 1.1 í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´

```bash
# Python 3.11+
python --version  # Python 3.11.0 ì´ìƒ

# Node.js 18+ (Dashboardìš©)
node --version    # v18.0.0 ì´ìƒ

# PostgreSQL 15+
psql --version    # PostgreSQL 15.0 ì´ìƒ

# Git
git --version

# Docker & Docker Compose
docker --version
docker-compose --version
```

### 1.2 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
f2x-neurohub-mes/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ alembic/               # DB ë§ˆì´ê·¸ë ˆì´ì…˜
â”‚   â”‚   â”œâ”€â”€ versions/
â”‚   â”‚   â””â”€â”€ env.py
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI ì•±
â”‚   â”‚   â”œâ”€â”€ core/              # í•µì‹¬ ì„¤ì •
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â”‚   â””â”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ models/            # SQLAlchemy ëª¨ë¸
â”‚   â”‚   â”‚   â”œâ”€â”€ lot.py
â”‚   â”‚   â”‚   â”œâ”€â”€ serial.py
â”‚   â”‚   â”‚   â””â”€â”€ process.py
â”‚   â”‚   â”œâ”€â”€ schemas/           # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ lot.py
â”‚   â”‚   â”‚   â””â”€â”€ process.py
â”‚   â”‚   â”œâ”€â”€ api/               # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lots.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ serials.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ processes.py
â”‚   â”‚   â”‚   â””â”€â”€ deps.py        # ì˜ì¡´ì„±
â”‚   â”‚   â”œâ”€â”€ services/          # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”‚   â”‚   â”œâ”€â”€ lot_service.py
â”‚   â”‚   â”‚   â””â”€â”€ process_service.py
â”‚   â”‚   â””â”€â”€ utils/             # ìœ í‹¸ë¦¬í‹°
â”‚   â”‚       â”œâ”€â”€ errors.py
â”‚   â”‚       â””â”€â”€ validators.py
â”‚   â”œâ”€â”€ tests/                 # í…ŒìŠ¤íŠ¸
â”‚   â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ conftest.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ requirements-dev.txt
â”‚   â””â”€â”€ alembic.ini
â”‚
â”œâ”€â”€ frontend-pc/               # PyQt5 ì‘ì—… PC ì•±
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ main_window.py
â”‚   â”‚   â””â”€â”€ process_forms/
â”‚   â”‚       â”œâ”€â”€ spring_input.py
â”‚   â”‚       â”œâ”€â”€ lma_assembly.py
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ api_client.py
â”‚   â”‚   â””â”€â”€ offline_queue.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend-dashboard/        # React ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Lots.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Reports.tsx
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”‚
â”œâ”€â”€ docker/                    # Docker ì„¤ì •
â”‚   â”œâ”€â”€ backend.Dockerfile
â”‚   â”œâ”€â”€ frontend.Dockerfile
â”‚   â””â”€â”€ nginx.conf
â”‚
â”œâ”€â”€ scripts/                   # ìš´ì˜ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ backup.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â””â”€â”€ init-db.sql
â”‚
â”œâ”€â”€ docs/                      # ë¬¸ì„œ
â”‚   â”œâ”€â”€ API.md
â”‚   â””â”€â”€ DEPLOYMENT.md
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.dev.yml
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

### 1.3 ë¡œì»¬ ê°œë°œ í™˜ê²½ êµ¬ì¶•

#### 1.3.1 Backend ì„¤ì •

```bash
# 1. ê°€ìƒ í™˜ê²½ ìƒì„±
cd backend
python -m venv venv

# 2. ê°€ìƒ í™˜ê²½ í™œì„±í™”
# Windows
venv\Scripts\activate
# Mac/Linux
source venv/bin/activate

# 3. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install -r requirements-dev.txt

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ í¸ì§‘

# 5. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜
alembic upgrade head

# 6. ê°œë°œ ì„œë²„ ì‹¤í–‰
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### 1.3.2 í™˜ê²½ ë³€ìˆ˜ (.env)

```bash
# .env
# Database
DATABASE_URL=postgresql://mes_user:mes_password@localhost:5432/mes_db

# JWT
SECRET_KEY=your-secret-key-change-in-production-min-32-chars
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=60

# CORS
BACKEND_CORS_ORIGINS=["http://localhost:3000", "http://localhost:5173"]

# Environment
ENVIRONMENT=development  # development, staging, production

# Logging
LOG_LEVEL=INFO
```

#### 1.3.3 Frontend Dashboard ì„¤ì •

```bash
cd frontend-dashboard

# 1. ì˜ì¡´ì„± ì„¤ì¹˜
npm install

# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env.local
# .env.local í¸ì§‘

# 3. ê°œë°œ ì„œë²„ ì‹¤í–‰
npm run dev  # http://localhost:5173
```

#### 1.3.4 Docker Composeë¡œ ì „ì²´ í™˜ê²½ ì‹¤í–‰

```bash
# ê°œë°œ í™˜ê²½ ì „ì²´ ì‹¤í–‰
docker-compose -f docker-compose.dev.yml up

# ì„œë¹„ìŠ¤:
# - PostgreSQL: localhost:5432
# - Backend API: http://localhost:8000
# - Dashboard: http://localhost:3000
# - pgAdmin: http://localhost:5050
```

**docker-compose.dev.yml:**

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: mes_user
      POSTGRES_PASSWORD: mes_password
      POSTGRES_DB: mes_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  backend:
    build:
      context: ./backend
      dockerfile: ../docker/backend.Dockerfile
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://mes_user:mes_password@postgres:5432/mes_db
      REDIS_URL: redis://redis:6379
    depends_on:
      - postgres
      - redis
    volumes:
      - ./backend:/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  pgadmin:
    image: dpage/pgadmin4
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@mes.local
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"

volumes:
  postgres_data:
```

---

## 2. ë°ì´í„°ë² ì´ìŠ¤ êµ¬í˜„

### 2.1 ì „ì²´ DDL ìŠ¤í¬ë¦½íŠ¸

```sql
-- scripts/init-db.sql

-- ============================================
-- 1. ê³µì • ë§ˆìŠ¤í„°
-- ============================================
CREATE TABLE processes (
    id SERIAL PRIMARY KEY,
    process_code VARCHAR(20) UNIQUE NOT NULL,
    process_name VARCHAR(100) NOT NULL,
    sequence_order INTEGER NOT NULL,
    description TEXT,
    standard_cycle_time INTEGER,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ê¸°ë³¸ ë°ì´í„°
INSERT INTO processes (process_code, process_name, sequence_order, standard_cycle_time) VALUES
('SPRING', 'ìŠ¤í”„ë§ íˆ¬ì…', 1, 120),
('LMA', 'LMA ì¡°ë¦½', 2, 180),
('LASER', 'ë ˆì´ì € ë§ˆí‚¹', 3, 60),
('EOL', 'EOL ê²€ì‚¬', 4, 300),
('ROBOT', 'ë¡œë´‡ ì„±ëŠ¥ê²€ì‚¬', 5, 180),
('PRINT', 'í”„ë¦°íŒ…', 6, 60),
('PACK', 'í¬ì¥', 7, 120);

CREATE INDEX idx_processes_sequence ON processes(sequence_order);

-- ============================================
-- 2. ì œí’ˆ ëª¨ë¸ ë§ˆìŠ¤í„°
-- ============================================
CREATE TABLE product_models (
    id SERIAL PRIMARY KEY,
    model_code VARCHAR(50) UNIQUE NOT NULL,
    model_name VARCHAR(100) NOT NULL,
    specification JSONB,
    target_cycle_time INTEGER,
    bom JSONB,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ê¸°ë³¸ ë°ì´í„°
INSERT INTO product_models (model_code, model_name, specification, target_cycle_time) VALUES
('NH-F2X-001', 'F2X Wearable Robot Standard',
 '{"weight": "2.5kg", "battery": "Li-ion 5000mAh", "color": "Black"}'::jsonb,
 900);

CREATE INDEX idx_product_models_code ON product_models(model_code);

-- ============================================
-- 3. LOT ì •ë³´
-- ============================================
CREATE TABLE lots (
    id BIGSERIAL PRIMARY KEY,
    lot_number VARCHAR(50) UNIQUE NOT NULL,

    plant_code VARCHAR(10) NOT NULL,
    product_model_id INTEGER NOT NULL REFERENCES product_models(id),
    shift VARCHAR(1) NOT NULL CHECK (shift IN ('D', 'N')),
    production_date DATE NOT NULL,

    target_quantity INTEGER NOT NULL CHECK (target_quantity > 0),
    actual_quantity INTEGER DEFAULT 0 CHECK (actual_quantity >= 0),
    defect_quantity INTEGER DEFAULT 0 CHECK (defect_quantity >= 0),

    status VARCHAR(20) NOT NULL DEFAULT 'CREATED',
    priority VARCHAR(20) DEFAULT 'NORMAL',

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    created_by VARCHAR(50),
    notes TEXT,

    CONSTRAINT check_quantity CHECK (actual_quantity + defect_quantity <= target_quantity)
);

CREATE INDEX idx_lots_lot_number ON lots(lot_number);
CREATE INDEX idx_lots_status ON lots(status);
CREATE INDEX idx_lots_plant_date ON lots(plant_code, production_date);
CREATE INDEX idx_lots_priority ON lots(priority, created_at);
CREATE INDEX idx_lots_created_at ON lots(created_at DESC);

-- ============================================
-- 4. ì‹œë¦¬ì–¼ ë²ˆí˜¸
-- ============================================
CREATE TABLE serials (
    id BIGSERIAL PRIMARY KEY,
    serial_number VARCHAR(100) UNIQUE NOT NULL,

    lot_id BIGINT NOT NULL REFERENCES lots(id) ON DELETE CASCADE,
    sequence_in_lot INTEGER NOT NULL,
    checksum VARCHAR(2) NOT NULL,

    status VARCHAR(20) NOT NULL DEFAULT 'CREATED',
    current_process_id INTEGER REFERENCES processes(id),

    is_defective BOOLEAN DEFAULT FALSE,
    defect_code VARCHAR(50),
    defect_description TEXT,

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,

    UNIQUE(lot_id, sequence_in_lot)
);

CREATE INDEX idx_serials_serial_number ON serials(serial_number);
CREATE INDEX idx_serials_lot_id ON serials(lot_id);
CREATE INDEX idx_serials_status ON serials(status);
CREATE INDEX idx_serials_created_at ON serials(created_at DESC);
CREATE INDEX idx_serials_defective ON serials(is_defective) WHERE is_defective = TRUE;

-- ============================================
-- 5. í†µí•© ê³µì • ë°ì´í„°
-- ============================================
CREATE TABLE process_data (
    id BIGSERIAL PRIMARY KEY,

    serial_id BIGINT NOT NULL REFERENCES serials(id),
    process_id INTEGER NOT NULL REFERENCES processes(id),
    work_order INTEGER NOT NULL DEFAULT 1,

    started_at TIMESTAMP WITH TIME ZONE NOT NULL,
    completed_at TIMESTAMP WITH TIME ZONE,
    cycle_time INTEGER,

    operator_id VARCHAR(50),
    equipment_id VARCHAR(50),

    status VARCHAR(20) NOT NULL DEFAULT 'IN_PROGRESS',
    is_pass BOOLEAN,

    process_specific_data JSONB,
    inspection_result JSONB,

    defect_code VARCHAR(50),
    defect_description TEXT,

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    CONSTRAINT unique_serial_process_order UNIQUE(serial_id, process_id, work_order)
);

CREATE INDEX idx_process_data_serial ON process_data(serial_id);
CREATE INDEX idx_process_data_process ON process_data(process_id);
CREATE INDEX idx_process_data_started ON process_data(started_at DESC);
CREATE INDEX idx_process_data_status ON process_data(status);
CREATE INDEX idx_process_data_operator ON process_data(operator_id);
CREATE INDEX idx_process_data_jsonb_gin ON process_data USING GIN (process_specific_data);

-- ============================================
-- 6. ìƒíƒœ ë³€ê²½ ì´ë ¥
-- ============================================
CREATE TABLE status_history (
    id BIGSERIAL PRIMARY KEY,
    entity_type VARCHAR(20) NOT NULL,
    entity_id BIGINT NOT NULL,
    old_status VARCHAR(20),
    new_status VARCHAR(20) NOT NULL,
    changed_by VARCHAR(50),
    changed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    reason TEXT
);

CREATE INDEX idx_status_history_entity ON status_history(entity_type, entity_id);
CREATE INDEX idx_status_history_changed_at ON status_history(changed_at DESC);

-- ============================================
-- 7. ê°ì‚¬ ë¡œê·¸
-- ============================================
CREATE TABLE audit_log (
    id BIGSERIAL PRIMARY KEY,
    table_name VARCHAR(50) NOT NULL,
    record_id BIGINT NOT NULL,
    action VARCHAR(10) NOT NULL,
    old_data JSONB,
    new_data JSONB,
    changed_by VARCHAR(50),
    changed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    ip_address INET,
    user_agent TEXT
);

CREATE INDEX idx_audit_log_table ON audit_log(table_name, record_id);
CREATE INDEX idx_audit_log_changed_at ON audit_log(changed_at DESC);
CREATE INDEX idx_audit_log_changed_by ON audit_log(changed_by);

-- ============================================
-- 8. ì‚¬ìš©ì ë° ê¶Œí•œ
-- ============================================
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    full_name VARCHAR(100),
    role VARCHAR(20) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE permissions (
    id SERIAL PRIMARY KEY,
    role VARCHAR(20) NOT NULL,
    resource VARCHAR(50) NOT NULL,
    action VARCHAR(20) NOT NULL,
    UNIQUE(role, resource, action)
);

-- ê¸°ë³¸ ê¶Œí•œ ì„¤ì •
INSERT INTO permissions (role, resource, action) VALUES
('OPERATOR', 'PROCESS', 'CREATE'),
('OPERATOR', 'PROCESS', 'READ'),
('SUPERVISOR', 'PROCESS', 'CREATE'),
('SUPERVISOR', 'PROCESS', 'READ'),
('SUPERVISOR', 'PROCESS', 'UPDATE'),
('SUPERVISOR', 'REPORT', 'READ'),
('SUPERVISOR', 'LOT', 'CREATE'),
('SUPERVISOR', 'LOT', 'READ'),
('ADMIN', 'LOT', 'CREATE'),
('ADMIN', 'LOT', 'READ'),
('ADMIN', 'LOT', 'UPDATE'),
('ADMIN', 'LOT', 'DELETE'),
('ADMIN', 'SERIAL', 'CREATE'),
('ADMIN', 'SERIAL', 'READ'),
('ADMIN', 'SERIAL', 'UPDATE'),
('ADMIN', 'SERIAL', 'DELETE'),
('ADMIN', 'USER', 'CREATE'),
('ADMIN', 'USER', 'READ'),
('ADMIN', 'USER', 'UPDATE'),
('ADMIN', 'USER', 'DELETE'),
('ADMIN', 'REPORT', 'READ');

-- ============================================
-- 9. ë¶ˆëŸ‰ ì½”ë“œ ë§ˆìŠ¤í„°
-- ============================================
CREATE TABLE defect_codes (
    id SERIAL PRIMARY KEY,
    defect_code VARCHAR(50) UNIQUE NOT NULL,
    defect_name VARCHAR(100) NOT NULL,
    process_id INTEGER REFERENCES processes(id),
    severity VARCHAR(20),
    description TEXT,
    is_active BOOLEAN DEFAULT TRUE
);

INSERT INTO defect_codes (defect_code, defect_name, process_id, severity) VALUES
('D001', 'ìŠ¤í”„ë§ ë¶ˆëŸ‰', 1, 'MAJOR'),
('D002', 'LMA ì¡°ë¦½ ë¶ˆëŸ‰', 2, 'CRITICAL'),
('D003', 'ë§ˆí‚¹ í’ˆì§ˆ ë¶ˆëŸ‰', 3, 'MINOR'),
('D004', 'ì˜¨ë„ì„¼ì„œ ì´ìƒ', 4, 'CRITICAL'),
('D005', 'TOF ì„¼ì„œ ì´ìƒ', 4, 'CRITICAL'),
('D006', 'íŒì›¨ì–´ ì—…ë¡œë“œ ì‹¤íŒ¨', 4, 'CRITICAL'),
('D007', 'ë¡œë´‡ ë™ì‘ ë¶ˆëŸ‰', 5, 'CRITICAL'),
('D008', 'í”„ë¦°íŒ… ë¶ˆëŸ‰', 6, 'MINOR');

CREATE INDEX idx_defect_codes_process ON defect_codes(process_id);

-- ============================================
-- 10. ì¬ì‘ì—… (Rework)
-- ============================================
CREATE TABLE reworks (
    id BIGSERIAL PRIMARY KEY,
    serial_id BIGINT NOT NULL REFERENCES serials(id),
    original_process_id INTEGER NOT NULL REFERENCES processes(id),
    defect_code VARCHAR(50) NOT NULL,
    rework_reason TEXT,
    rework_started_at TIMESTAMP WITH TIME ZONE,
    rework_completed_at TIMESTAMP WITH TIME ZONE,
    rework_operator VARCHAR(50),
    is_completed BOOLEAN DEFAULT FALSE,
    final_result VARCHAR(20),
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_reworks_serial ON reworks(serial_id);
CREATE INDEX idx_reworks_process ON reworks(original_process_id);
CREATE INDEX idx_reworks_completed ON reworks(is_completed);

-- ============================================
-- 11. ì—ëŸ¬ ë¡œê·¸
-- ============================================
CREATE TABLE error_logs (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    error_code VARCHAR(50) NOT NULL,
    severity VARCHAR(20) NOT NULL,
    message TEXT NOT NULL,
    context JSONB,
    stack_trace TEXT,
    resolved BOOLEAN DEFAULT FALSE,
    resolved_at TIMESTAMP WITH TIME ZONE,
    resolved_by VARCHAR(50),
    notes TEXT
);

CREATE INDEX idx_error_logs_timestamp ON error_logs(timestamp DESC);
CREATE INDEX idx_error_logs_severity ON error_logs(severity);
CREATE INDEX idx_error_logs_resolved ON error_logs(resolved) WHERE resolved = FALSE;

-- ============================================
-- 12. ì ‘ê·¼ ë¡œê·¸
-- ============================================
CREATE TABLE access_logs (
    id BIGSERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    username VARCHAR(50),
    resource VARCHAR(50) NOT NULL,
    action VARCHAR(20) NOT NULL,
    ip_address INET,
    user_agent TEXT,
    status_code INTEGER,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_access_logs_user ON access_logs(user_id);
CREATE INDEX idx_access_logs_timestamp ON access_logs(timestamp DESC);

-- ============================================
-- íŠ¸ë¦¬ê±° í•¨ìˆ˜
-- ============================================

-- ê°ì‚¬ ë¡œê·¸ íŠ¸ë¦¬ê±° í•¨ìˆ˜
CREATE OR REPLACE FUNCTION audit_trigger_func()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        INSERT INTO audit_log (table_name, record_id, action, new_data, changed_by)
        VALUES (TG_TABLE_NAME, NEW.id, 'INSERT', row_to_json(NEW)::jsonb, current_user);
        RETURN NEW;
    ELSIF TG_OP = 'UPDATE' THEN
        INSERT INTO audit_log (table_name, record_id, action, old_data, new_data, changed_by)
        VALUES (TG_TABLE_NAME, NEW.id, 'UPDATE',
                row_to_json(OLD)::jsonb, row_to_json(NEW)::jsonb, current_user);
        RETURN NEW;
    ELSIF TG_OP = 'DELETE' THEN
        INSERT INTO audit_log (table_name, record_id, action, old_data, changed_by)
        VALUES (TG_TABLE_NAME, OLD.id, 'DELETE', row_to_json(OLD)::jsonb, current_user);
        RETURN OLD;
    END IF;
END;
$$ LANGUAGE plpgsql;

-- ìƒíƒœ ë³€ê²½ ì´ë ¥ íŠ¸ë¦¬ê±° í•¨ìˆ˜
CREATE OR REPLACE FUNCTION status_history_trigger_func()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'UPDATE' AND OLD.status IS DISTINCT FROM NEW.status THEN
        INSERT INTO status_history (entity_type, entity_id, old_status, new_status, changed_by)
        VALUES (TG_TABLE_NAME, NEW.id, OLD.status, NEW.status, current_user);
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- íŠ¸ë¦¬ê±° ì ìš©
CREATE TRIGGER lots_audit_trigger
    AFTER INSERT OR UPDATE OR DELETE ON lots
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_func();

CREATE TRIGGER serials_audit_trigger
    AFTER INSERT OR UPDATE OR DELETE ON serials
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_func();

CREATE TRIGGER lots_status_history_trigger
    AFTER UPDATE ON lots
    FOR EACH ROW EXECUTE FUNCTION status_history_trigger_func();

CREATE TRIGGER serials_status_history_trigger
    AFTER UPDATE ON serials
    FOR EACH ROW EXECUTE FUNCTION status_history_trigger_func();

-- ============================================
-- ë·° (Views)
-- ============================================

-- ìƒì‚° í˜„í™© ìš”ì•½ ë·°
CREATE OR REPLACE VIEW v_production_summary AS
SELECT
    l.id AS lot_id,
    l.lot_number,
    l.production_date,
    l.shift,
    pm.model_name AS product_model,
    l.target_quantity,
    l.actual_quantity,
    l.defect_quantity,
    ROUND(l.actual_quantity::numeric / NULLIF(l.target_quantity, 0) * 100, 2) AS completion_rate,
    ROUND(l.defect_quantity::numeric / NULLIF(l.actual_quantity, 0) * 100, 2) AS defect_rate,
    l.status,
    l.created_at,
    l.started_at,
    l.completed_at
FROM lots l
JOIN product_models pm ON l.product_model_id = pm.id
ORDER BY l.created_at DESC;

-- ê³µì •ë³„ ì§„í–‰ í˜„í™© ë·°
CREATE OR REPLACE VIEW v_process_progress AS
SELECT
    p.process_name,
    COUNT(DISTINCT pd.serial_id) AS total_processed,
    COUNT(DISTINCT CASE WHEN pd.is_pass = TRUE THEN pd.serial_id END) AS passed,
    COUNT(DISTINCT CASE WHEN pd.is_pass = FALSE THEN pd.serial_id END) AS failed,
    ROUND(AVG(pd.cycle_time), 2) AS avg_cycle_time,
    MIN(pd.cycle_time) AS min_cycle_time,
    MAX(pd.cycle_time) AS max_cycle_time
FROM process_data pd
JOIN processes p ON pd.process_id = p.id
WHERE pd.started_at >= CURRENT_DATE
GROUP BY p.id, p.process_name
ORDER BY p.sequence_order;
```

### 2.2 Alembic ë§ˆì´ê·¸ë ˆì´ì…˜

```python
# backend/alembic/versions/001_initial.py
"""Initial migration

Revision ID: 001
Create Date: 2025-11-10

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '001'
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    # ì—¬ê¸°ì— ìœ„ DDL ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš©ì„ Python ì½”ë“œë¡œ ë³€í™˜
    # (ì‹¤ì œë¡œëŠ” scripts/init-db.sqlì„ ì§ì ‘ ì‹¤í–‰í•˜ê±°ë‚˜,
    #  Alembicìœ¼ë¡œ í…Œì´ë¸” ìƒì„± ì½”ë“œ ì‘ì„±)

    # ì˜ˆì‹œ: processes í…Œì´ë¸”
    op.create_table(
        'processes',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('process_code', sa.String(20), nullable=False),
        sa.Column('process_name', sa.String(100), nullable=False),
        sa.Column('sequence_order', sa.Integer(), nullable=False),
        sa.Column('description', sa.Text(), nullable=True),
        sa.Column('standard_cycle_time', sa.Integer(), nullable=True),
        sa.Column('is_active', sa.Boolean(), server_default='true'),
        sa.Column('created_at', sa.TIMESTAMP(timezone=True),
                  server_default=sa.text('now()')),
        sa.Column('updated_at', sa.TIMESTAMP(timezone=True),
                  server_default=sa.text('now()')),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('process_code')
    )

    # ... (ë‚˜ë¨¸ì§€ í…Œì´ë¸”ë“¤)

def downgrade():
    op.drop_table('processes')
    # ... (ë‚˜ë¨¸ì§€ í…Œì´ë¸”ë“¤)
```

---

## 3. Backend API êµ¬í˜„

### 3.1 í”„ë¡œì íŠ¸ ì„¤ì •

#### 3.1.1 requirements.txt

```txt
# FastAPI
fastapi==0.109.0
uvicorn[standard]==0.27.0
pydantic==2.5.3
pydantic-settings==2.1.0

# Database
sqlalchemy==2.0.25
asyncpg==0.29.0
alembic==1.13.1
psycopg2-binary==2.9.9

# Authentication
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6

# Redis
redis==5.0.1

# Utilities
python-dateutil==2.8.2
pytz==2023.3
```

#### 3.1.2 requirements-dev.txt

```txt
# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0
httpx==0.26.0

# Linting & Formatting
black==23.12.1
flake8==7.0.0
mypy==1.8.0
isort==5.13.2

# Development
ipython==8.20.0
```

### 3.2 í•µì‹¬ ëª¨ë“ˆ êµ¬í˜„

#### 3.2.1 Database ì„¤ì • (backend/app/core/database.py)

```python
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from .config import settings

# Async engine
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=settings.ENVIRONMENT == "development",
    future=True,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20
)

# Async session factory
AsyncSessionLocal = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

Base = declarative_base()

async def get_db():
    """FastAPI dependency for database session"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()
```

#### 3.2.2 Config ì„¤ì • (backend/app/core/config.py)

```python
from pydantic_settings import BaseSettings
from typing import List

class Settings(BaseSettings):
    # Application
    PROJECT_NAME: str = "F2X NeuroHub MES"
    VERSION: str = "2.0.0"
    API_V1_STR: str = "/api/v1"
    ENVIRONMENT: str = "development"

    # Database
    DATABASE_URL: str

    # Security
    SECRET_KEY: str
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60

    # CORS
    BACKEND_CORS_ORIGINS: List[str] = [
        "http://localhost:3000",
        "http://localhost:5173"
    ]

    # Redis
    REDIS_URL: str = "redis://localhost:6379"

    # Logging
    LOG_LEVEL: str = "INFO"

    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

#### 3.2.3 Security (backend/app/core/security.py)

```python
from datetime import datetime, timedelta
from typing import Optional
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from .config import settings

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
security = HTTPBearer()

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """ë¹„ë°€ë²ˆí˜¸ í•´ì‹±"""
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    """JWT í† í° ìƒì„±"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(
            minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES
        )
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(
        to_encode, settings.SECRET_KEY, algorithm=settings.ALGORITHM
    )
    return encoded_jwt

def decode_token(token: str) -> dict:
    """JWT í† í° ë””ì½”ë”©"""
    try:
        payload = jwt.decode(
            token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM]
        )
        return payload
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials"
        )

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> dict:
    """í˜„ì¬ ì‚¬ìš©ì ì¡°íšŒ (Dependency)"""
    token = credentials.credentials
    payload = decode_token(token)

    username: str = payload.get("sub")
    if username is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials"
        )

    return payload
```

#### 3.2.4 LOT Model (backend/app/models/lot.py)

```python
from sqlalchemy import Column, Integer, String, Date, TIMESTAMP, Boolean, Text, BigInteger
from sqlalchemy.sql import func
from sqlalchemy.orm import relationship
from app.core.database import Base

class Lot(Base):
    __tablename__ = "lots"

    id = Column(BigInteger, primary_key=True, index=True)
    lot_number = Column(String(50), unique=True, nullable=False, index=True)

    plant_code = Column(String(10), nullable=False)
    product_model_id = Column(Integer, nullable=False)
    shift = Column(String(1), nullable=False)
    production_date = Column(Date, nullable=False)

    target_quantity = Column(Integer, nullable=False)
    actual_quantity = Column(Integer, default=0)
    defect_quantity = Column(Integer, default=0)

    status = Column(String(20), nullable=False, default='CREATED', index=True)
    priority = Column(String(20), default='NORMAL')

    created_at = Column(TIMESTAMP(timezone=True), server_default=func.now())
    started_at = Column(TIMESTAMP(timezone=True))
    completed_at = Column(TIMESTAMP(timezone=True))
    created_by = Column(String(50))
    notes = Column(Text)

    # Relationships
    serials = relationship("Serial", back_populates="lot", cascade="all, delete-orphan")
```

#### 3.2.5 LOT Schema (backend/app/schemas/lot.py)

```python
from pydantic import BaseModel, Field, validator
from typing import Optional
from datetime import date, datetime

class LotCreate(BaseModel):
    plant_code: str = Field(..., max_length=10, description="ê³µì¥ ì½”ë“œ")
    product_model_code: str = Field(..., max_length=50, description="ì œí’ˆ ëª¨ë¸ ì½”ë“œ")
    shift: str = Field(..., pattern=r'^[DN]$', description="êµëŒ€ (D/N)")
    target_quantity: int = Field(..., gt=0, description="ëª©í‘œ ìˆ˜ëŸ‰")
    priority: str = Field(default="NORMAL", pattern=r'^(URGENT|HIGH|NORMAL|LOW)$')

    @validator('shift')
    def validate_shift(cls, v):
        if v not in ['D', 'N']:
            raise ValueError('êµëŒ€ëŠ” D(ì£¼ê°„) ë˜ëŠ” N(ì•¼ê°„)ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤')
        return v

class LotResponse(BaseModel):
    id: int
    lot_number: str
    plant_code: str
    production_date: date
    shift: str
    target_quantity: int
    actual_quantity: int
    defect_quantity: int
    status: str
    priority: str
    created_at: datetime
    started_at: Optional[datetime]
    completed_at: Optional[datetime]

    class Config:
        from_attributes = True

class LotDetail(LotResponse):
    """LOT ìƒì„¸ ì •ë³´ (ì‹œë¦¬ì–¼ í¬í•¨)"""
    serials: list  # Simplified, ì‹¤ì œë¡œëŠ” SerialResponse ì‚¬ìš©
    completion_rate: float
    defect_rate: float

    @validator('completion_rate', always=True)
    def calc_completion_rate(cls, v, values):
        target = values.get('target_quantity', 0)
        actual = values.get('actual_quantity', 0)
        return round(actual / target * 100, 2) if target > 0 else 0.0

    @validator('defect_rate', always=True)
    def calc_defect_rate(cls, v, values):
        actual = values.get('actual_quantity', 0)
        defect = values.get('defect_quantity', 0)
        return round(defect / actual * 100, 2) if actual > 0 else 0.0
```

#### 3.2.6 LOT Service (backend/app/services/lot_service.py)

```python
from datetime import date, datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from app.models.lot import Lot
from app.models.product import ProductModel
from app.schemas.lot import LotCreate

class LotService:
    @staticmethod
    async def generate_lot_number(
        db: AsyncSession,
        plant_code: str,
        shift: str
    ) -> str:
        """LOT ë²ˆí˜¸ ìƒì„±"""
        today = date.today().strftime("%Y%m%d")

        # ì˜¤ëŠ˜ í•´ë‹¹ ê³µì¥/êµëŒ€ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ì¡°íšŒ
        result = await db.execute(
            select(func.max(Lot.lot_number)).where(
                Lot.plant_code == plant_code,
                Lot.production_date == date.today(),
                Lot.shift == shift
            )
        )
        max_lot = result.scalar()

        if max_lot:
            # ê¸°ì¡´ LOTê°€ ìˆìœ¼ë©´ ì‹œí€€ìŠ¤ ì¦ê°€
            last_seq = int(max_lot.split('-')[-1])
            new_seq = last_seq + 1
        else:
            # ì²« LOT
            new_seq = 1

        lot_number = f"FN-{plant_code}-{today}-{shift}-{new_seq:06d}"
        return lot_number

    @staticmethod
    async def create_lot(
        db: AsyncSession,
        lot_data: LotCreate,
        current_user: dict
    ) -> Lot:
        """LOT ìƒì„±"""
        # 1. ì œí’ˆ ëª¨ë¸ ì¡°íšŒ
        product = await db.execute(
            select(ProductModel).where(
                ProductModel.model_code == lot_data.product_model_code
            )
        )
        product_model = product.scalar_one_or_none()
        if not product_model:
            raise ValueError(f"ì œí’ˆ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lot_data.product_model_code}")

        # 2. LOT ë²ˆí˜¸ ìƒì„±
        lot_number = await LotService.generate_lot_number(
            db, lot_data.plant_code, lot_data.shift
        )

        # 3. LOT ìƒì„±
        new_lot = Lot(
            lot_number=lot_number,
            plant_code=lot_data.plant_code,
            product_model_id=product_model.id,
            shift=lot_data.shift,
            production_date=date.today(),
            target_quantity=lot_data.target_quantity,
            priority=lot_data.priority,
            status='CREATED',
            created_by=current_user.get('sub')
        )

        db.add(new_lot)
        await db.commit()
        await db.refresh(new_lot)

        return new_lot

    @staticmethod
    async def get_lot_by_number(
        db: AsyncSession,
        lot_number: str
    ) -> Lot:
        """LOT ë²ˆí˜¸ë¡œ ì¡°íšŒ"""
        result = await db.execute(
            select(Lot).where(Lot.lot_number == lot_number)
        )
        lot = result.scalar_one_or_none()
        if not lot:
            raise ValueError(f"LOTë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lot_number}")
        return lot
```

#### 3.2.7 LOT API (backend/app/api/v1/lots.py)

```python
from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.ext.asyncio import AsyncSession
from typing import List
from app.core.database import get_db
from app.core.security import get_current_user
from app.schemas.lot import LotCreate, LotResponse, LotDetail
from app.services.lot_service import LotService

router = APIRouter()

@router.post("/", response_model=LotResponse, status_code=status.HTTP_201_CREATED)
async def create_lot(
    lot_data: LotCreate,
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    ìƒˆ LOT ìƒì„±

    - **plant_code**: ê³µì¥ ì½”ë“œ (ì˜ˆ: KR01)
    - **product_model_code**: ì œí’ˆ ëª¨ë¸ ì½”ë“œ
    - **shift**: êµëŒ€ (D=Day, N=Night)
    - **target_quantity**: ëª©í‘œ ìˆ˜ëŸ‰
    """
    try:
        lot = await LotService.create_lot(db, lot_data, current_user)
        return lot
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )

@router.get("/{lot_number}", response_model=LotDetail)
async def get_lot(
    lot_number: str,
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """LOT ìƒì„¸ ì¡°íšŒ"""
    try:
        lot = await LotService.get_lot_by_number(db, lot_number)
        return lot
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e)
        )

@router.get("/", response_model=List[LotResponse])
async def list_lots(
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    status: str = Query(None),
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """LOT ëª©ë¡ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜)"""
    # êµ¬í˜„ ìƒëµ (LotServiceì— list_lots ë©”ì„œë“œ ì¶”ê°€)
    pass
```

#### 3.2.8 Main App (backend/app/main.py)

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.core.config import settings
from app.api.v1 import lots, serials, processes, auth

app = FastAPI(
    title=settings.PROJECT_NAME,
    version=settings.VERSION,
    openapi_url=f"{settings.API_V1_STR}/openapi.json"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.BACKEND_CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API Routes
app.include_router(auth.router, prefix=f"{settings.API_V1_STR}/auth", tags=["auth"])
app.include_router(lots.router, prefix=f"{settings.API_V1_STR}/lots", tags=["lots"])
app.include_router(serials.router, prefix=f"{settings.API_V1_STR}/serials", tags=["serials"])
app.include_router(processes.router, prefix=f"{settings.API_V1_STR}/processes", tags=["processes"])

@app.get("/")
async def root():
    return {
        "project": settings.PROJECT_NAME,
        "version": settings.VERSION,
        "environment": settings.ENVIRONMENT
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy"}
```

---

## 4. Frontend êµ¬í˜„

### 4.1 PyQt5 ì‘ì—… PC ì•±

#### 4.1.1 ë©”ì¸ ìœˆë„ìš° (frontend-pc/ui/main_window.py)

```python
from PyQt5.QtWidgets import (
    QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,
    QPushButton, QLabel, QLineEdit, QComboBox, QTextEdit,
    QStatusBar, QMessageBox
)
from PyQt5.QtCore import Qt, QTimer
from services.api_client import APIClient
from services.offline_queue import OfflineQueue

class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.api_client = APIClient()
        self.offline_queue = OfflineQueue()

        self.init_ui()
        self.setup_barcode_scanner()
        self.start_queue_processor()

    def init_ui(self):
        self.setWindowTitle("F2X MES - ê³µì • ë°ì´í„° ì…ë ¥")
        self.setGeometry(100, 100, 800, 600)

        # Central Widget
        central_widget = QWidget()
        self.setCentralWidget(central_widget)

        # Main Layout
        layout = QVBoxLayout()
        central_widget.setLayout(layout)

        # ì‹œë¦¬ì–¼ ë²ˆí˜¸ ì…ë ¥
        serial_layout = QHBoxLayout()
        serial_layout.addWidget(QLabel("ì‹œë¦¬ì–¼ ë²ˆí˜¸:"))
        self.serial_input = QLineEdit()
        self.serial_input.setPlaceholderText("ë°”ì½”ë“œ ìŠ¤ìº” ë˜ëŠ” ì§ì ‘ ì…ë ¥")
        self.serial_input.returnPressed.connect(self.on_serial_scanned)
        serial_layout.addWidget(self.serial_input)
        layout.addLayout(serial_layout)

        # ê³µì • ì„ íƒ
        process_layout = QHBoxLayout()
        process_layout.addWidget(QLabel("ê³µì •:"))
        self.process_combo = QComboBox()
        self.process_combo.addItems([
            "ìŠ¤í”„ë§ íˆ¬ì…", "LMA ì¡°ë¦½", "ë ˆì´ì € ë§ˆí‚¹",
            "EOL ê²€ì‚¬", "ë¡œë´‡ ì„±ëŠ¥ê²€ì‚¬", "í”„ë¦°íŒ…", "í¬ì¥"
        ])
        process_layout.addWidget(self.process_combo)
        layout.addLayout(process_layout)

        # ë°ì´í„° ì…ë ¥ ì˜ì—­ (ë™ì  ìƒì„±)
        self.data_input_widget = QWidget()
        self.data_input_layout = QVBoxLayout()
        self.data_input_widget.setLayout(self.data_input_layout)
        layout.addWidget(self.data_input_widget)

        # ë²„íŠ¼
        button_layout = QHBoxLayout()
        self.start_btn = QPushButton("ì°©ê³µ")
        self.start_btn.clicked.connect(self.on_start_process)
        self.complete_btn = QPushButton("ì™„ê³µ")
        self.complete_btn.clicked.connect(self.on_complete_process)
        button_layout.addWidget(self.start_btn)
        button_layout.addWidget(self.complete_btn)
        layout.addLayout(button_layout)

        # ë¡œê·¸
        self.log_text = QTextEdit()
        self.log_text.setReadOnly(True)
        self.log_text.setMaximumHeight(150)
        layout.addWidget(QLabel("ë¡œê·¸:"))
        layout.addWidget(self.log_text)

        # Status Bar
        self.status_bar = QStatusBar()
        self.setStatusBar(self.status_bar)
        self.update_connection_status()

    def setup_barcode_scanner(self):
        """ë°”ì½”ë“œ ìŠ¤ìºë„ˆ ì„¤ì • (í‚¤ë³´ë“œ ì…ë ¥ ê°ì§€)"""
        self.barcode_buffer = ""
        self.barcode_timer = QTimer()
        self.barcode_timer.setSingleShot(True)
        self.barcode_timer.timeout.connect(self.reset_barcode_buffer)

    def keyPressEvent(self, event):
        """ë°”ì½”ë“œ ìŠ¤ìºë„ˆëŠ” ë¹ ë¥¸ ì†ë„ë¡œ í‚¤ ì…ë ¥"""
        if event.key() == Qt.Key_Return:
            if len(self.barcode_buffer) > 5:  # ë°”ì½”ë“œë¡œ íŒë‹¨
                self.serial_input.setText(self.barcode_buffer)
                self.on_serial_scanned()
                self.reset_barcode_buffer()
        else:
            self.barcode_buffer += event.text()
            self.barcode_timer.start(100)  # 100ms í›„ ë²„í¼ ë¦¬ì…‹

    def reset_barcode_buffer(self):
        self.barcode_buffer = ""

    def on_serial_scanned(self):
        """ì‹œë¦¬ì–¼ ë²ˆí˜¸ ìŠ¤ìº” ì²˜ë¦¬"""
        serial_number = self.serial_input.text().strip()
        if not serial_number:
            return

        self.log(f"ì‹œë¦¬ì–¼ ë²ˆí˜¸ ìŠ¤ìº”: {serial_number}")

        # ì‹œë¦¬ì–¼ ì •ë³´ ì¡°íšŒ
        try:
            serial_info = self.api_client.get_serial(serial_number)
            self.log(f"í˜„ì¬ ê³µì •: {serial_info['current_process']}")
            # ê³µì • ìë™ ì„ íƒ
            # self.process_combo.setCurrentText(serial_info['current_process'])
        except Exception as e:
            self.log(f"ì˜¤ë¥˜: {str(e)}")
            # ì˜¤í”„ë¼ì¸ ëª¨ë“œ
            self.log("ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì‘ì—…í•©ë‹ˆë‹¤")

    def on_start_process(self):
        """
        ê³µì • ì°©ê³µ (ë°”ì½”ë“œ ìŠ¤ìº” â†’ API í˜¸ì¶œ)

        âœ… ì°©ê³µ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤:
        1. ì‘ì—…ì ë°”ì½”ë“œ ìŠ¤ìº” â†’ operator_id
        2. ì œí’ˆ ì‹œë¦¬ì–¼ ë²ˆí˜¸ ë°”ì½”ë“œ ìŠ¤ìº” â†’ serial_number
        3. ìë™ ì •ë³´ ìˆ˜ì§‘ (process_code, equipment_id, workstation)
        4. Backend API í˜¸ì¶œ â†’ ì¦‰ì‹œ ì‘ë‹µ
        5. UI í”¼ë“œë°± í‘œì‹œ
        """
        serial_number = self.serial_input.text().strip()
        process = self.process_combo.currentText()

        if not serial_number:
            QMessageBox.warning(self, "ê²½ê³ ", "ì‹œë¦¬ì–¼ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            return

        # ì°©ê³µ ë°ì´í„° êµ¬ì„±
        data = {
            "serial_number": serial_number,
            "process_code": self.get_process_code(process),
            "operator_id": self.config.get("operator_id", "OPERATOR01"),  # ì‹¤ì œë¡œëŠ” ë¡œê·¸ì¸ ì •ë³´ ë˜ëŠ” ë°”ì½”ë“œ ìŠ¤ìº”
            "equipment_id": self.config.get("equipment_id"),  # ì‘ì—… PC ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ (ì„ íƒ)
            "workstation": self.config.get("workstation")     # ì‘ì—… PC ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ (ì„ íƒ)
        }

        try:
            # ë™ê¸° API í˜¸ì¶œ (ì¦‰ì‹œ ì‘ë‹µ)
            result = self.api_client.start_process(data)
            self.log(f"ì°©ê³µ ì™„ë£Œ: {result['data']}")
            QMessageBox.information(
                self,
                "ì°©ê³µ ì„±ê³µ",
                f"ê³µì • ì°©ê³µì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\nì‹œë¦¬ì–¼: {serial_number}\nì‹œì‘ ì‹œê°: {result['data']['started_at']}"
            )
        except Exception as e:
            # ì˜¤í”„ë¼ì¸ íì— ì €ì¥
            self.offline_queue.enqueue("/api/v1/process/start", "POST", data)
            self.log(f"ì˜¤í”„ë¼ì¸ íì— ì €ì¥: {data}")
            QMessageBox.warning(self, "ì˜¤í”„ë¼ì¸ ëª¨ë“œ", "ì„œë²„ ì—°ê²° ì‹¤íŒ¨. ì¬ì—°ê²° ì‹œ ìë™ ì „ì†¡ë©ë‹ˆë‹¤")

    def on_complete_process(self):
        """ê³µì • ì™„ê³µ"""
        # êµ¬í˜„ ìƒëµ (on_start_processì™€ ìœ ì‚¬)
        pass

    def get_process_code(self, process_name):
        """ê³µì • ì´ë¦„ â†’ ì½”ë“œ ë³€í™˜"""
        mapping = {
            "ìŠ¤í”„ë§ íˆ¬ì…": "SPRING",
            "LMA ì¡°ë¦½": "LMA",
            "ë ˆì´ì € ë§ˆí‚¹": "LASER",
            "EOL ê²€ì‚¬": "EOL",
            "ë¡œë´‡ ì„±ëŠ¥ê²€ì‚¬": "ROBOT",
            "í”„ë¦°íŒ…": "PRINT",
            "í¬ì¥": "PACK"
        }
        return mapping.get(process_name, "UNKNOWN")

    def log(self, message):
        """ë¡œê·¸ ì¶”ê°€"""
        from datetime import datetime
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_text.append(f"[{timestamp}] {message}")

    def update_connection_status(self):
        """ì—°ê²° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            self.api_client.health_check()
            self.status_bar.showMessage("ğŸŸ¢ ì„œë²„ ì—°ê²°ë¨", 5000)
        except:
            self.status_bar.showMessage("ğŸ”´ ì„œë²„ ì—°ê²° ëŠê¹€ (ì˜¤í”„ë¼ì¸ ëª¨ë“œ)", 5000)

        # 5ì´ˆë§ˆë‹¤ ì²´í¬
        QTimer.singleShot(5000, self.update_connection_status)

    def start_queue_processor(self):
        """ì˜¤í”„ë¼ì¸ í ì²˜ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ)"""
        def process_queue():
            self.offline_queue.process_queue(self.api_client)
            # 10ì´ˆë§ˆë‹¤ ì²´í¬
            QTimer.singleShot(10000, process_queue)

        process_queue()
```

#### 4.1.2 API Client (frontend-pc/services/api_client.py)

```python
import requests
from typing import Dict, Any

class APIClient:
    def __init__(self, base_url="http://192.168.1.100:8000/api/v1"):
        self.base_url = base_url
        self.token = None
        self.session = requests.Session()

    def login(self, username: str, password: str):
        """ë¡œê·¸ì¸"""
        response = self.session.post(
            f"{self.base_url}/auth/login",
            json={"username": username, "password": password}
        )
        response.raise_for_status()
        data = response.json()
        self.token = data["data"]["access_token"]
        self.session.headers.update({
            "Authorization": f"Bearer {self.token}"
        })

    def health_check(self):
        """í—¬ìŠ¤ ì²´í¬"""
        response = self.session.get(f"{self.base_url}/../health", timeout=2)
        response.raise_for_status()

    def get_serial(self, serial_number: str) -> Dict[str, Any]:
        """ì‹œë¦¬ì–¼ ì¡°íšŒ"""
        response = self.session.get(f"{self.base_url}/serials/{serial_number}")
        response.raise_for_status()
        return response.json()["data"]

    def start_process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê³µì • ì°©ê³µ"""
        response = self.session.post(f"{self.base_url}/process/start", json=data)
        response.raise_for_status()
        return response.json()["data"]

    def complete_process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ê³µì • ì™„ê³µ"""
        response = self.session.post(f"{self.base_url}/process/complete", json=data)
        response.raise_for_status()
        return response.json()["data"]
```

#### 4.1.3 Offline Queue (frontend-pc/services/offline_queue.py)

```python
import sqlite3
import json
from datetime import datetime
from typing import Dict, Any

class OfflineQueue:
    def __init__(self, db_path="offline_queue.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS queue (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                endpoint VARCHAR(200) NOT NULL,
                method VARCHAR(10) NOT NULL,
                payload TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                retry_count INTEGER DEFAULT 0,
                last_error TEXT,
                status VARCHAR(20) DEFAULT 'PENDING'
            )
        """)
        conn.commit()
        conn.close()

    def enqueue(self, endpoint: str, method: str, payload: Dict[str, Any]):
        """íì— ì¶”ê°€"""
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            """
            INSERT INTO queue (endpoint, method, payload)
            VALUES (?, ?, ?)
            """,
            (endpoint, method, json.dumps(payload))
        )
        conn.commit()
        conn.close()

    def process_queue(self, api_client):
        """í ì²˜ë¦¬"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute(
            """
            SELECT id, endpoint, method, payload
            FROM queue
            WHERE status = 'PENDING' AND retry_count < 5
            ORDER BY created_at
            LIMIT 100
            """
        )

        for row in cursor.fetchall():
            queue_id, endpoint, method, payload = row
            try:
                # ì„œë²„ë¡œ ì „ì†¡ ì‹œë„
                if method == "POST":
                    api_client.session.post(
                        f"{api_client.base_url}{endpoint}",
                        json=json.loads(payload)
                    ).raise_for_status()

                # ì„±ê³µ ì‹œ íì—ì„œ ì œê±°
                conn.execute("DELETE FROM queue WHERE id = ?", (queue_id,))
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
                conn.execute(
                    """
                    UPDATE queue
                    SET retry_count = retry_count + 1,
                        last_error = ?,
                        status = CASE WHEN retry_count >= 4 THEN 'FAILED' ELSE 'PENDING' END
                    WHERE id = ?
                    """,
                    (str(e), queue_id)
                )

        conn.commit()
        conn.close()
```

#### 4.1.4 JSON íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ (frontend-pc/services/json_file_monitor.py)

**âš ï¸ ì¤‘ìš”: ì™¸ë¶€ ê³µì • ì•±ê³¼ì˜ í†µì‹  - ì™„ê³µ(COMPLETE) ì „ìš©**

ì´ ì„œë¹„ìŠ¤ëŠ” ì™¸ë¶€ ì—…ì²´ê°€ ê°œë°œí•œ ê³µì • ì•±(7ê°œ, ê°ê¸° ë‹¤ë¦„)ì´ ìƒì„±í•˜ëŠ” **ì™„ê³µ(COMPLETE) JSON íŒŒì¼**ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.

**ì°©ê³µ vs ì™„ê³µ ì²˜ë¦¬ ë°©ì‹:**
- âœ… **ì°©ê³µ(START)**: **ë°”ì½”ë“œ ìŠ¤ìºë„ˆ + UI ì§ì ‘ ì…ë ¥** (ì£¼ìš” ë°©ë²•, Section 4.1.1 ì°¸ì¡°)
  - ì‘ì—…ìê°€ ë°”ì½”ë“œ ë¦¬ë”ê¸°ë¡œ LOT ìŠ¤ìº” â†’ ì¦‰ì‹œ UI í”¼ë“œë°±
  - JSON íŒŒì¼ ëª¨ë‹ˆí„°ë§ì€ ë°±ì—… ìš©ë„ë¡œë§Œ ì‚¬ìš© ê°€ëŠ¥ (ì„ íƒì‚¬í•­)
- âœ… **ì™„ê³µ(COMPLETE)**: **JSON íŒŒì¼ ëª¨ë‹ˆí„°ë§** (ì´ ì„¹ì…˜ì˜ ì£¼ìš” ëª©ì )
  - ê³µì • ì•±ì€ ìˆ˜ì • ë¶ˆê°€ëŠ¥ (ì†ŒìŠ¤ì½”ë“œ ì ‘ê·¼ ë¶ˆê°€)
  - JSON íŒŒì¼ ë°©ì‹ì´ ìœ ì¼í•œ í†µì‹  ìˆ˜ë‹¨
  - í´ë” ìœ„ì¹˜: `C:\F2X\input\complete\`

**ì„¤ê³„ ê·¼ê±°:**
- ì°©ê³µ ì‹œì—ëŠ” ì‘ì—…ìê°€ PC ì•ì— ìˆì–´ ì¦‰ê°ì ì¸ í”¼ë“œë°±ì´ ê°€ëŠ¥í•˜ê³  í•„ìš”í•¨
- ì™„ê³µ ì‹œì—ëŠ” ì™¸ë¶€ ê³µì • ì•±ì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë¹„ë™ê¸° íŒŒì¼ ëª¨ë‹ˆí„°ë§ì´ ì í•©í•¨

```python
import os
import json
import shutil
import time
import msvcrt
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from jsonschema import validate, ValidationError
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('json_monitor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class JSONFileMonitor:
    """JSON íŒŒì¼ ëª¨ë‹ˆí„°ë§ ë° ì²˜ë¦¬ ì„œë¹„ìŠ¤"""

    # JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
    START_SCHEMA = {
        "type": "object",
        "required": ["serial_number", "process_code", "operator_id", "equipment_id", "timestamp"],
        "properties": {
            "serial_number": {"type": "string", "minLength": 10},
            "process_code": {"type": "string", "enum": ["SPRING", "LMA", "LASER", "EOL", "ROBOT", "PRINT", "PACK"]},
            "operator_id": {"type": "string"},
            "equipment_id": {"type": "string"},
            "timestamp": {"type": "string", "format": "date-time"}
        }
    }

    COMPLETE_SCHEMA = {
        "type": "object",
        "required": ["serial_number", "process_code", "operator_id", "is_pass", "timestamp"],
        "properties": {
            "serial_number": {"type": "string", "minLength": 10},
            "process_code": {"type": "string", "enum": ["SPRING", "LMA", "LASER", "EOL", "ROBOT", "PRINT", "PACK"]},
            "operator_id": {"type": "string"},
            "is_pass": {"type": "boolean"},
            "cycle_time": {"type": "number"},
            "process_specific_data": {"type": "object"},
            "inspection_result": {"type": "object"},
            "defect_code": {"type": ["string", "null"]},
            "timestamp": {"type": "string", "format": "date-time"}
        }
    }

    def __init__(self, api_client, offline_queue):
        """
        Args:
            api_client: APIClient ì¸ìŠ¤í„´ìŠ¤
            offline_queue: OfflineQueue ì¸ìŠ¤í„´ìŠ¤
        """
        self.api_client = api_client
        self.offline_queue = offline_queue

        # í´ë” ê²½ë¡œ ì„¤ì •
        self.base_path = Path("C:/F2X")
        self.input_start_path = self.base_path / "input" / "start"
        self.input_complete_path = self.base_path / "input" / "complete"
        self.processed_path = self.base_path / "processed"
        self.error_path = self.base_path / "error"

        # í´ë” ìƒì„±
        self._create_folders()

        # watchdog ì„¤ì •
        self.observer = Observer()

    def _create_folders(self):
        """í•„ìš”í•œ í´ë” ìƒì„±"""
        for path in [self.input_start_path, self.input_complete_path,
                     self.processed_path, self.error_path]:
            path.mkdir(parents=True, exist_ok=True)
        logger.info(f"í´ë” êµ¬ì¡° ìƒì„± ì™„ë£Œ: {self.base_path}")

    def start(self):
        """íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""

        # [ì„ íƒì‚¬í•­] start í´ë” ëª¨ë‹ˆí„°ë§ (ë°±ì—…ìš©)
        # ì£¼ì˜: ì°©ê³µì€ ë°”ì½”ë“œ UIê°€ ì£¼ìš” ë°©ë²•ì´ë©°, JSON ëª¨ë‹ˆí„°ë§ì€ ë°±ì—… ìš©ë„
        # í•„ìš”í•˜ì§€ ì•Šë‹¤ë©´ ì´ ë¶€ë¶„ì„ ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥
        start_handler = JSONFileHandler(
            monitor=self,
            operation_type='START',
            schema=self.START_SCHEMA
        )
        self.observer.schedule(start_handler, str(self.input_start_path), recursive=False)

        # [í•„ìˆ˜] complete í´ë” ëª¨ë‹ˆí„°ë§ (ì£¼ìš” ê¸°ëŠ¥)
        # ì™¸ë¶€ ê³µì • ì•±ì´ ìƒì„±í•˜ëŠ” ì™„ê³µ JSON íŒŒì¼ ì²˜ë¦¬
        complete_handler = JSONFileHandler(
            monitor=self,
            operation_type='COMPLETE',
            schema=self.COMPLETE_SCHEMA
        )
        self.observer.schedule(complete_handler, str(self.input_complete_path), recursive=False)

        self.observer.start()
        logger.info("JSON íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        logger.info(f"  - START: {self.input_start_path} (ë°±ì—…ìš©, ì„ íƒì‚¬í•­)")
        logger.info(f"  - COMPLETE: {self.input_complete_path} (ì£¼ìš” ê¸°ëŠ¥, í•„ìˆ˜)")

    def stop(self):
        """íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        self.observer.stop()
        self.observer.join()
        logger.info("JSON íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")

    def read_json_file_safe(self, file_path: Path, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """
        íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì½ê¸° (íŒŒì¼ ë½ ì²˜ë¦¬)

        Args:
            file_path: ì½ì„ íŒŒì¼ ê²½ë¡œ
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            íŒŒì‹±ëœ JSON ë°ì´í„° ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        for attempt in range(max_retries):
            try:
                # íŒŒì¼ì´ ì™„ì „íˆ ì“°ì—¬ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸° (í¬ê¸° ì²´í¬)
                initial_size = file_path.stat().st_size
                time.sleep(0.1)
                current_size = file_path.stat().st_size

                if initial_size != current_size:
                    logger.debug(f"íŒŒì¼ ì“°ê¸° ì§„í–‰ ì¤‘, ëŒ€ê¸°... ({file_path.name})")
                    time.sleep(0.5)
                    continue

                # íŒŒì¼ ì½ê¸° (Windows íŒŒì¼ ë½ ê³ ë ¤)
                with open(file_path, 'r', encoding='utf-8') as f:
                    # Windowsì—ì„œ íŒŒì¼ ë½ ì‹œë„
                    if os.name == 'nt':
                        try:
                            msvcrt.locking(f.fileno(), msvcrt.LK_NBLCK, 1)
                        except IOError:
                            logger.warning(f"íŒŒì¼ ë½ íšë“ ì‹¤íŒ¨, ì¬ì‹œë„... ({file_path.name})")
                            time.sleep(0.5)
                            continue

                    content = f.read()

                    # íŒŒì¼ ë½ í•´ì œ
                    if os.name == 'nt':
                        try:
                            msvcrt.locking(f.fileno(), msvcrt.LK_UNLCK, 1)
                        except:
                            pass

                # JSON íŒŒì‹±
                data = json.loads(content)
                logger.info(f"íŒŒì¼ ì½ê¸° ì„±ê³µ: {file_path.name}")
                return data

            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {file_path.name} - {str(e)}")
                return None
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                time.sleep(0.5)

        logger.error(f"íŒŒì¼ ì½ê¸° ìµœì¢… ì‹¤íŒ¨: {file_path.name}")
        return None

    def validate_json(self, data: Dict[str, Any], schema: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """
        JSON ë°ì´í„° ê²€ì¦

        Returns:
            (ê²€ì¦ ì„±ê³µ ì—¬ë¶€, ì—ëŸ¬ ë©”ì‹œì§€)
        """
        try:
            validate(instance=data, schema=schema)
            return True, None
        except ValidationError as e:
            return False, str(e)

    def process_json_file(self, file_path: Path, operation_type: str, schema: Dict[str, Any]):
        """
        JSON íŒŒì¼ ì²˜ë¦¬ ë©”ì¸ ë¡œì§

        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
            operation_type: 'START' ë˜ëŠ” 'COMPLETE'
            schema: ê²€ì¦í•  JSON ìŠ¤í‚¤ë§ˆ
        """
        logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path.name} (íƒ€ì…: {operation_type})")

        try:
            # 1. íŒŒì¼ ì½ê¸°
            data = self.read_json_file_safe(file_path)
            if data is None:
                raise Exception("íŒŒì¼ ì½ê¸° ì‹¤íŒ¨")

            # 2. JSON ìŠ¤í‚¤ë§ˆ ê²€ì¦
            is_valid, error_msg = self.validate_json(data, schema)
            if not is_valid:
                raise Exception(f"JSON ê²€ì¦ ì‹¤íŒ¨: {error_msg}")

            # 3. ì„œë²„ë¡œ ì „ì†¡
            endpoint = f"/api/v1/process/{'start' if operation_type == 'START' else 'complete'}"

            try:
                if operation_type == 'START':
                    result = self.api_client.start_process(data)
                else:
                    result = self.api_client.complete_process(data)

                logger.info(f"ì„œë²„ ì „ì†¡ ì„±ê³µ: {data['serial_number']} - {operation_type}")

                # 4. ì²˜ë¦¬ ì™„ë£Œ í´ë”ë¡œ ì´ë™
                self.move_to_processed(file_path, operation_type)

            except Exception as e:
                # ì„œë²„ ì—°ê²° ì‹¤íŒ¨ â†’ ì˜¤í”„ë¼ì¸ íì— ì €ì¥
                logger.warning(f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨, ì˜¤í”„ë¼ì¸ í ì €ì¥: {str(e)}")
                self.offline_queue.enqueue(endpoint, "POST", data)

                # ì²˜ë¦¬ ì™„ë£Œ í´ë”ë¡œ ì´ë™ (íì— ì €ì¥ë¨)
                self.move_to_processed(file_path, operation_type)

        except Exception as e:
            # íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ â†’ error í´ë”ë¡œ ì´ë™
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {file_path.name} - {str(e)}")
            self.move_to_error(file_path, operation_type, str(e))

    def move_to_processed(self, file_path: Path, operation_type: str):
        """ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼ì„ processed í´ë”ë¡œ ì´ë™"""
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            new_name = f"{operation_type}_{timestamp}_{file_path.name}"
            dest_path = self.processed_path / new_name

            shutil.move(str(file_path), str(dest_path))
            logger.info(f"íŒŒì¼ ì´ë™ ì™„ë£Œ: {file_path.name} â†’ processed/{new_name}")
        except Exception as e:
            logger.error(f"íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {str(e)}")

    def move_to_error(self, file_path: Path, operation_type: str, error_msg: str):
        """ì—ëŸ¬ ë°œìƒí•œ íŒŒì¼ì„ error í´ë”ë¡œ ì´ë™"""
        try:
            # ì—ëŸ¬ ì •ë³´ íŒŒì¼ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            new_name = f"{operation_type}_{timestamp}_{file_path.name}"
            dest_path = self.error_path / new_name
            error_info_path = self.error_path / f"{new_name}.error.txt"

            # ì›ë³¸ íŒŒì¼ ì´ë™
            shutil.move(str(file_path), str(dest_path))

            # ì—ëŸ¬ ì •ë³´ ì €ì¥
            with open(error_info_path, 'w', encoding='utf-8') as f:
                f.write(f"Error Time: {datetime.now().isoformat()}\n")
                f.write(f"Operation Type: {operation_type}\n")
                f.write(f"Original File: {file_path.name}\n")
                f.write(f"Error Message:\n{error_msg}\n")

            logger.info(f"ì—ëŸ¬ íŒŒì¼ ì´ë™: {file_path.name} â†’ error/{new_name}")
        except Exception as e:
            logger.error(f"ì—ëŸ¬ íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {str(e)}")


class JSONFileHandler(FileSystemEventHandler):
    """watchdog íŒŒì¼ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""

    def __init__(self, monitor: JSONFileMonitor, operation_type: str, schema: Dict[str, Any]):
        super().__init__()
        self.monitor = monitor
        self.operation_type = operation_type
        self.schema = schema

    def on_created(self, event):
        """íŒŒì¼ ìƒì„± ì´ë²¤íŠ¸"""
        if event.is_directory:
            return

        file_path = Path(event.src_path)

        # JSON íŒŒì¼ë§Œ ì²˜ë¦¬
        if file_path.suffix.lower() != '.json':
            logger.debug(f"JSON íŒŒì¼ ì•„ë‹˜, ë¬´ì‹œ: {file_path.name}")
            return

        # ì„ì‹œ íŒŒì¼ ë¬´ì‹œ (ì˜ˆ: ~ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼)
        if file_path.name.startswith('~') or file_path.name.startswith('.'):
            logger.debug(f"ì„ì‹œ íŒŒì¼ ë¬´ì‹œ: {file_path.name}")
            return

        logger.info(f"ìƒˆ JSON íŒŒì¼ ê°ì§€: {file_path.name} (íƒ€ì…: {self.operation_type})")

        # íŒŒì¼ ì²˜ë¦¬ (ì•½ê°„ì˜ ì§€ì—° í›„ - íŒŒì¼ ì“°ê¸° ì™„ë£Œ ëŒ€ê¸°)
        time.sleep(0.2)
        self.monitor.process_json_file(file_path, self.operation_type, self.schema)
```

#### 4.1.5 ë©”ì¸ ì•±ì— JSON ëª¨ë‹ˆí„°ë§ í†µí•© (frontend-pc/main.py)

```python
import sys
from PyQt5.QtWidgets import QApplication
from ui.main_window import MainWindow
from services.api_client import APIClient
from services.offline_queue import OfflineQueue
from services.json_file_monitor import JSONFileMonitor
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    """Frontend App ë©”ì¸ ì§„ì…ì """
    app = QApplication(sys.argv)

    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    api_client = APIClient()
    offline_queue = OfflineQueue()

    # JSON íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    json_monitor = JSONFileMonitor(api_client, offline_queue)
    json_monitor.start()

    logger.info("=== F2X MES Frontend App ì‹œì‘ ===")
    logger.info("- ì°©ê³µ ë°©ì‹: ë°”ì½”ë“œ ìŠ¤ìºë„ˆ + UI (ì£¼ìš”)")
    logger.info("- ì™„ê³µ ë°©ì‹: JSON íŒŒì¼ ëª¨ë‹ˆí„°ë§")
    logger.info("  â†’ ì™„ê³µ í´ë”: C:\\F2X\\input\\complete\\ (ì£¼ìš” ê°ì‹œ)")
    logger.info("  â†’ ì°©ê³µ í´ë”: C:\\F2X\\input\\start\\ (ë°±ì—…ìš©, ì„ íƒ)")

    # ë©”ì¸ ìœˆë„ìš° ì‹¤í–‰
    window = MainWindow()
    window.api_client = api_client
    window.offline_queue = offline_queue
    window.json_monitor = json_monitor
    window.show()

    # ì¢…ë£Œ ì‹œ ëª¨ë‹ˆí„°ë§ ì •ë¦¬
    exit_code = app.exec_()
    json_monitor.stop()

    logger.info("=== F2X MES Frontend App ì¢…ë£Œ ===")
    sys.exit(exit_code)


if __name__ == '__main__':
    main()
```

#### 4.1.6 ì™¸ë¶€ ê³µì • ì•± ê°œë°œ ê°€ì´ë“œ

**ì™¸ë¶€ ì—…ì²´ìš© JSON íŒŒì¼ ì‘ì„± ê°€ì´ë“œ**

```python
# ì™¸ë¶€ ê³µì • ì•± ì˜ˆì‹œ ì½”ë“œ (ì°¸ê³ ìš©)
# ì‹¤ì œ êµ¬í˜„ì€ ê° ì—…ì²´ì˜ ê°œë°œ í™˜ê²½ì— ë§ê²Œ ì‘ì„±

import json
import os
import tempfile
from datetime import datetime
from pathlib import Path


def write_start_json(serial_number: str, process_code: str, operator_id: str, equipment_id: str):
    """
    ê³µì • ì°©ê³µ JSON íŒŒì¼ ì‘ì„±

    âš ï¸ ì¤‘ìš”:
    - íŒŒì¼ ìœ„ì¹˜: C:\F2X\input\start\
    - íŒŒì¼ëª…: ììœ  (ì˜ˆ: {ê³µì •}_{timestamp}.json, ë˜ëŠ” ì„ì˜)
    - ì›ìì  ì“°ê¸°: ì„ì‹œ íŒŒì¼ â†’ rename (íŒŒì¼ ë½ ë°©ì§€)
    """
    # 1. JSON ë°ì´í„° ìƒì„±
    data = {
        "serial_number": serial_number,
        "process_code": process_code,
        "operator_id": operator_id,
        "equipment_id": equipment_id,
        "timestamp": datetime.now().astimezone().isoformat()
    }

    # 2. ëŒ€ìƒ í´ë”
    target_dir = Path("C:/F2X/input/start")
    target_dir.mkdir(parents=True, exist_ok=True)

    # 3. ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì“°ê¸° (ì›ìì  ì“°ê¸°)
    temp_file = tempfile.NamedTemporaryFile(
        mode='w',
        encoding='utf-8',
        suffix='.json',
        dir=str(target_dir),
        delete=False
    )

    try:
        json.dump(data, temp_file, ensure_ascii=False, indent=2)
        temp_file.close()

        # 4. ìµœì¢… íŒŒì¼ëª…ìœ¼ë¡œ rename
        final_filename = f"{process_code}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        final_path = target_dir / final_filename

        os.rename(temp_file.name, str(final_path))
        print(f"ì°©ê³µ JSON íŒŒì¼ ìƒì„± ì™„ë£Œ: {final_path}")

    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
        raise e


def write_complete_json(serial_number: str, process_code: str, operator_id: str,
                       is_pass: bool, cycle_time: int, process_data: dict):
    """
    ê³µì • ì™„ê³µ JSON íŒŒì¼ ì‘ì„±

    âš ï¸ ì¤‘ìš”:
    - íŒŒì¼ ìœ„ì¹˜: C:\F2X\input\complete\
    - process_data: ê³µì •ë³„ íŠ¹í™” ë°ì´í„° (JSON ê°ì²´)
    """
    data = {
        "serial_number": serial_number,
        "process_code": process_code,
        "operator_id": operator_id,
        "is_pass": is_pass,
        "cycle_time": cycle_time,
        "process_specific_data": process_data,
        "inspection_result": {},  # ê²€ì‚¬ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¶”ê°€
        "defect_code": None if is_pass else "DEFECT_CODE_HERE",
        "timestamp": datetime.now().astimezone().isoformat()
    }

    target_dir = Path("C:/F2X/input/complete")
    target_dir.mkdir(parents=True, exist_ok=True)

    # ì„ì‹œ íŒŒì¼ â†’ rename (ë™ì¼í•œ íŒ¨í„´)
    temp_file = tempfile.NamedTemporaryFile(
        mode='w',
        encoding='utf-8',
        suffix='.json',
        dir=str(target_dir),
        delete=False
    )

    try:
        json.dump(data, temp_file, ensure_ascii=False, indent=2)
        temp_file.close()

        final_filename = f"{process_code}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        final_path = target_dir / final_filename

        os.rename(temp_file.name, str(final_path))
        print(f"ì™„ê³µ JSON íŒŒì¼ ìƒì„± ì™„ë£Œ: {final_path}")

    except Exception as e:
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
        raise e


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    # ì°©ê³µ ì˜ˆì‹œ
    write_start_json(
        serial_number="FN-KR-251110D-001-0001",
        process_code="LMA",
        operator_id="W002",
        equipment_id="LMA-STATION-01"
    )

    # ì™„ê³µ ì˜ˆì‹œ
    write_complete_json(
        serial_number="FN-KR-251110D-001-0001",
        process_code="LMA",
        operator_id="W002",
        is_pass=True,
        cycle_time=185,
        process_data={
            "lma_model": "LMA-2024-V2",
            "assembly_complete": True,
            "torque_test": 5.2
        }
    )
```

**ì™¸ë¶€ ì—…ì²´ ì²´í¬ë¦¬ìŠ¤íŠ¸**

âœ… **í•„ìˆ˜ êµ¬í˜„ ì‚¬í•­**
- [ ] UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ JSON íŒŒì¼ ì‘ì„±
- [ ] ì°©ê³µ: `C:\F2X\input\start\` í´ë”ì— íŒŒì¼ ìƒì„±
- [ ] ì™„ê³µ: `C:\F2X\input\complete\` í´ë”ì— íŒŒì¼ ìƒì„±
- [ ] ì„ì‹œ íŒŒì¼ â†’ rename íŒ¨í„´ ì‚¬ìš© (ì›ìì  ì“°ê¸°)
- [ ] JSON ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ (í•„ìˆ˜ í•„ë“œ ëª¨ë‘ í¬í•¨)
- [ ] timestampëŠ” ISO 8601 í˜•ì‹ (timezone í¬í•¨)

âœ… **ê¶Œì¥ ì‚¬í•­**
- [ ] íŒŒì¼ëª…ì— ê³µì • ì½”ë“œ í¬í•¨ (ë””ë²„ê¹… ìš©ì´)
- [ ] íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ (ì¤‘ë³µ ë°©ì§€)
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
- [ ] ë¡œì»¬ ë¡œê·¸ íŒŒì¼ ì‘ì„± (ë¬¸ì œ ì¶”ì ìš©)

### 4.2 React Dashboard (ê°„ëµ)

#### 4.2.1 API Client (frontend-dashboard/src/api/client.ts)

```typescript
import axios, { AxiosInstance } from 'axios';

class APIClient {
  private client: AxiosInstance;
  private token: string | null = null;

  constructor(baseURL: string = 'http://192.168.1.100:8000/api/v1') {
    this.client = axios.create({
      baseURL,
      timeout: 10000,
    });

    // Request interceptor (í† í° ì¶”ê°€)
    this.client.interceptors.request.use((config) => {
      if (this.token) {
        config.headers.Authorization = `Bearer ${this.token}`;
      }
      return config;
    });

    // Response interceptor (ì—ëŸ¬ ì²˜ë¦¬)
    this.client.interceptors.response.use(
      (response) => response.data,
      (error) => {
        if (error.response?.status === 401) {
          // í† í° ë§Œë£Œ â†’ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ
          window.location.href = '/login';
        }
        return Promise.reject(error);
      }
    );
  }

  async login(username: string, password: string) {
    const response = await this.client.post('/auth/login', {
      username,
      password,
    });
    this.token = response.data.access_token;
    localStorage.setItem('token', this.token!);
    return response;
  }

  async getLots(page: number = 1, pageSize: number = 20) {
    return this.client.get('/lots', {
      params: { page, page_size: pageSize },
    });
  }

  async getDashboardSummary() {
    return this.client.get('/dashboard/summary');
  }
}

export default new APIClient();
```

#### 4.2.2 Dashboard Page (frontend-dashboard/src/pages/Dashboard.tsx)

```typescript
import React, { useEffect, useState } from 'react';
import { Card, Row, Col, Statistic } from 'antd';
import { Line } from '@ant-design/charts';
import apiClient from '../api/client';

const Dashboard: React.FC = () => {
  const [summary, setSummary] = useState<any>(null);

  useEffect(() => {
    loadSummary();
    // 10ì´ˆë§ˆë‹¤ ê°±ì‹ 
    const interval = setInterval(loadSummary, 10000);
    return () => clearInterval(interval);
  }, []);

  const loadSummary = async () => {
    const data = await apiClient.getDashboardSummary();
    setSummary(data.data);
  };

  if (!summary) return <div>Loading...</div>;

  return (
    <div>
      <h1>ìƒì‚° í˜„í™© ëŒ€ì‹œë³´ë“œ</h1>

      <Row gutter={16}>
        <Col span={6}>
          <Card>
            <Statistic
              title="ê¸ˆì¼ LOT ìˆ˜"
              value={summary.total_lots}
              suffix="ê°œ"
            />
          </Card>
        </Col>
        <Col span={6}>
          <Card>
            <Statistic
              title="ì™„ë£Œìœ¨"
              value={summary.completion_rate}
              suffix="%"
            />
          </Card>
        </Col>
        <Col span={6}>
          <Card>
            <Statistic
              title="ë¶ˆëŸ‰ë¥ "
              value={summary.defect_rate}
              suffix="%"
              valueStyle={{ color: '#cf1322' }}
            />
          </Card>
        </Col>
        <Col span={6}>
          <Card>
            <Statistic
              title="ì§„í–‰ ì¤‘"
              value={summary.active_lots}
              suffix="LOT"
            />
          </Card>
        </Col>
      </Row>

      {/* ê³µì •ë³„ ì°¨íŠ¸ ë“± ì¶”ê°€ */}
    </div>
  );
};

export default Dashboard;
```

---

## 5. ë³´ì•ˆ êµ¬í˜„

(ë‚´ìš©ì€ ì•ì„œ ì‘ì„±í•œ ë¶€ë¶„ê³¼ ìœ ì‚¬í•˜ë¯€ë¡œ ìƒëµ)

---

## 6. í…ŒìŠ¤íŠ¸ êµ¬í˜„

### 6.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ

```python
# tests/unit/test_lot_service.py
import pytest
from app.services.lot_service import LotService
from app.schemas.lot import LotCreate

@pytest.mark.asyncio
async def test_generate_lot_number(test_db):
    lot_number = await LotService.generate_lot_number(
        test_db, "KR01", "D"
    )
    assert lot_number.startswith("FN-KR01")
    assert "-D-" in lot_number

@pytest.mark.asyncio
async def test_create_lot(test_db, test_user):
    lot_data = LotCreate(
        plant_code="KR01",
        product_model_code="NH-F2X-001",
        shift="D",
        target_quantity=100
    )

    lot = await LotService.create_lot(test_db, lot_data, test_user)

    assert lot.id is not None
    assert lot.lot_number.startswith("FN-KR01")
    assert lot.target_quantity == 100
    assert lot.status == 'CREATED'
```

### 6.2 í†µí•© í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ

```python
# tests/integration/test_process_flow.py
@pytest.mark.asyncio
async def test_complete_process_flow(client, test_db):
    # 1. ë¡œê·¸ì¸
    login_response = await client.post("/api/v1/auth/login", json={
        "username": "operator01",
        "password": "password123"
    })
    token = login_response.json()["data"]["access_token"]
    headers = {"Authorization": f"Bearer {token}"}

    # 2. LOT ìƒì„±
    lot_response = await client.post("/api/v1/lots", json={
        "plant_code": "KR01",
        "product_model_code": "NH-F2X-001",
        "shift": "D",
        "target_quantity": 10
    }, headers=headers)
    lot_id = lot_response.json()["data"]["id"]

    # 3. ì‹œë¦¬ì–¼ ìƒì„±
    serial_response = await client.post(
        f"/api/v1/lots/{lot_id}/serials/generate",
        json={"quantity": 1},
        headers=headers
    )
    serial_number = serial_response.json()["data"]["serials"][0]

    # 4. 7ê°œ ê³µì • ìˆœì°¨ ì‹¤í–‰
    processes = ["SPRING", "LMA", "LASER", "EOL", "ROBOT", "PRINT", "PACK"]
    for process in processes:
        # ì°©ê³µ
        start_response = await client.post("/api/v1/process/start", json={
            "serial_number": serial_number,
            "process_code": process,
            "operator_id": "operator01"
        }, headers=headers)
        assert start_response.status_code == 201

        # ì™„ê³µ
        process_data_id = start_response.json()["data"]["process_data_id"]
        complete_response = await client.post("/api/v1/process/complete", json={
            "process_data_id": process_data_id,
            "is_pass": True,
            "process_specific_data": {}
        }, headers=headers)
        assert complete_response.status_code == 200

    # 5. ìµœì¢… ìƒíƒœ í™•ì¸
    final_response = await client.get(
        f"/api/v1/serials/{serial_number}",
        headers=headers
    )
    assert final_response.json()["data"]["status"] == "COMPLETED"
```

---

## 7. ë°°í¬ ë° ìš´ì˜

### 7.1 Docker ë°°í¬

```dockerfile
# docker/backend.Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì‚¬
COPY backend/ .

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# ì‹¤í–‰
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml (ìš´ì˜ í™˜ê²½)
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: docker/backend.Dockerfile
    environment:
      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}
      REDIS_URL: redis://redis:6379
      SECRET_KEY: ${SECRET_KEY}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./docker/nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  postgres_data:
```

### 7.2 ë°±ì—… ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# scripts/backup.sh

set -e

BACKUP_DIR="/backup/postgres"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="mes_db"

echo "=== Backup started at $(date) ==="

# PostgreSQL ë°±ì—…
pg_dump -h localhost -U mes_user -d $DB_NAME --format=custom \
    --file="$BACKUP_DIR/mes_db_$DATE.dump"

# ì••ì¶•
gzip "$BACKUP_DIR/mes_db_$DATE.dump"

# íŒŒì¼ ìŠ¤í† ë¦¬ì§€ ë°±ì—…
tar -czf "$BACKUP_DIR/storage_$DATE.tar.gz" /var/mes/storage/

# 30ì¼ ì´ìƒ ë°±ì—… ì‚­ì œ
find "$BACKUP_DIR" -name "*.gz" -mtime +30 -delete

echo "=== Backup completed at $(date) ==="
```

---

**END OF GUIDE**

ì´ êµ¬í˜„ ê°€ì´ë“œëŠ” ì‹¤ì œ ê°œë°œ ì‹œ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ì½”ë“œì™€ êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì€ ê° ê¸°ìˆ ì˜ ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
