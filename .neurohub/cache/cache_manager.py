"""
ë¬¸ì„œ ìºì‹œ ë§¤ë‹ˆì € (Document Cache Manager)

FR/AC/Design ë¬¸ì„œì˜ ì¤‘ë³µ ì½ê¸°ë¥¼ ë°©ì§€í•˜ê³  íŒŒì‹± ê²°ê³¼ë¥¼ ìºì‹±í•©ë‹ˆë‹¤.
- íŒŒì¼ ë³€ê²½ ê°ì§€ (mtime ê¸°ë°˜)
- ë©”ëª¨ë¦¬ ìºì‹± + ë””ìŠ¤í¬ ì˜ì†í™”
- ì—ì´ì „íŠ¸ ê°„ ìºì‹œ ê³µìœ 

ì‚¬ìš© ì˜ˆì‹œ:
    cache = CacheManager()

    # FR ë¬¸ì„œ ìºì‹±
    fr_content = cache.get_or_load('docs/requirements/modules/inventory/FR-INV-001.md')

    # íŒŒì‹±ëœ ê²°ê³¼ ìºì‹±
    parsed_data = cache.get_parsed('FR-INV-001')
    if not parsed_data:
        parsed_data = parse_fr_document(fr_content)
        cache.set_parsed('FR-INV-001', parsed_data)
"""

import os
import json
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime
import pickle


class CacheManager:
    """ë¬¸ì„œ ë° íŒŒì‹± ê²°ê³¼ ìºì‹œ ë§¤ë‹ˆì €"""

    def __init__(self, cache_dir: str = '.neurohub/cache'):
        """
        Args:
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.cache_dir = cache_dir
        self.memory_cache: Dict[str, Any] = {}
        self.metadata: Dict[str, Dict] = {}
        self._ensure_cache_dir()
        self._load_metadata()

    def _ensure_cache_dir(self):
        """ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(os.path.join(self.cache_dir, 'documents'), exist_ok=True)
        os.makedirs(os.path.join(self.cache_dir, 'parsed'), exist_ok=True)

    def _load_metadata(self):
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        metadata_file = os.path.join(self.cache_dir, 'metadata.json')
        if os.path.exists(metadata_file):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
            except Exception as e:
                print(f"âš ï¸  ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.metadata = {}
        else:
            self.metadata = {}

    def _save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata_file = os.path.join(self.cache_dir, 'metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)

    def _get_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        if not os.path.exists(file_path):
            return ''

        hasher = hashlib.sha256()
        with open(file_path, 'rb') as f:
            hasher.update(f.read())
        return hasher.hexdigest()

    def _get_cache_key(self, file_path: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # íŒŒì¼ ê²½ë¡œë¥¼ ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜
        safe_name = file_path.replace('/', '_').replace('\\', '_').replace(':', '')
        return safe_name

    def is_cache_valid(self, file_path: str) -> bool:
        """
        ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸ (íŒŒì¼ ë³€ê²½ ì—¬ë¶€)

        Args:
            file_path: í™•ì¸í•  íŒŒì¼ ê²½ë¡œ

        Returns:
            True if ìºì‹œ ìœ íš¨, False otherwise
        """
        cache_key = self._get_cache_key(file_path)

        if cache_key not in self.metadata:
            return False

        # íŒŒì¼ í•´ì‹œ ë¹„êµ
        current_hash = self._get_file_hash(file_path)
        cached_hash = self.metadata[cache_key].get('file_hash', '')

        return current_hash == cached_hash

    def get_or_load(self, file_path: str) -> Optional[str]:
        """
        íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ìš°ì„ )

        Args:
            file_path: ì½ì„ íŒŒì¼ ê²½ë¡œ

        Returns:
            íŒŒì¼ ë‚´ìš© (ë¬¸ìì—´) ë˜ëŠ” None
        """
        # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(file_path)

        if cache_key in self.memory_cache and self.is_cache_valid(file_path):
            print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìºì‹œ íˆíŠ¸: {file_path}")
            return self.memory_cache[cache_key]

        # 2. ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
        cache_file = os.path.join(self.cache_dir, 'documents', f'{cache_key}.txt')

        if os.path.exists(cache_file) and self.is_cache_valid(file_path):
            print(f"ğŸ’¿ ë””ìŠ¤í¬ ìºì‹œ íˆíŠ¸: {file_path}")
            with open(cache_file, 'r', encoding='utf-8') as f:
                content = f.read()
                self.memory_cache[cache_key] = content
                return content

        # 3. ìºì‹œ ë¯¸ìŠ¤ - íŒŒì¼ ì½ê¸°
        if not os.path.exists(file_path):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
            return None

        print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # 4. ìºì‹œ ì €ì¥
        self._cache_content(file_path, content)

        return content

    def _cache_content(self, file_path: str, content: str):
        """íŒŒì¼ ë‚´ìš©ì„ ìºì‹œì— ì €ì¥"""
        cache_key = self._get_cache_key(file_path)

        # ë©”ëª¨ë¦¬ ìºì‹œ
        self.memory_cache[cache_key] = content

        # ë””ìŠ¤í¬ ìºì‹œ
        cache_file = os.path.join(self.cache_dir, 'documents', f'{cache_key}.txt')
        with open(cache_file, 'w', encoding='utf-8') as f:
            f.write(content)

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        self.metadata[cache_key] = {
            'original_path': file_path,
            'file_hash': self._get_file_hash(file_path),
            'cached_at': datetime.now().isoformat(),
            'file_size': len(content)
        }
        self._save_metadata()

    def get_parsed(self, document_id: str) -> Optional[Dict]:
        """
        íŒŒì‹±ëœ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°

        Args:
            document_id: ë¬¸ì„œ ID (ì˜ˆ: 'FR-INV-001')

        Returns:
            íŒŒì‹±ëœ ë°ì´í„° (ë”•ì…”ë„ˆë¦¬) ë˜ëŠ” None
        """
        # ë©”ëª¨ë¦¬ ìºì‹œ
        mem_key = f'parsed_{document_id}'
        if mem_key in self.memory_cache:
            print(f"ğŸ’¾ íŒŒì‹± ìºì‹œ íˆíŠ¸ (ë©”ëª¨ë¦¬): {document_id}")
            return self.memory_cache[mem_key]

        # ë””ìŠ¤í¬ ìºì‹œ
        cache_file = os.path.join(self.cache_dir, 'parsed', f'{document_id}.pkl')
        if os.path.exists(cache_file):
            print(f"ğŸ’¿ íŒŒì‹± ìºì‹œ íˆíŠ¸ (ë””ìŠ¤í¬): {document_id}")
            try:
                with open(cache_file, 'rb') as f:
                    parsed_data = pickle.load(f)
                    self.memory_cache[mem_key] = parsed_data
                    return parsed_data
            except Exception as e:
                print(f"âš ï¸  íŒŒì‹± ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None

        return None

    def set_parsed(self, document_id: str, parsed_data: Dict):
        """
        íŒŒì‹±ëœ ë¬¸ì„œ ì €ì¥

        Args:
            document_id: ë¬¸ì„œ ID (ì˜ˆ: 'FR-INV-001')
            parsed_data: íŒŒì‹±ëœ ë°ì´í„° (ë”•ì…”ë„ˆë¦¬)
        """
        # ë©”ëª¨ë¦¬ ìºì‹œ
        mem_key = f'parsed_{document_id}'
        self.memory_cache[mem_key] = parsed_data

        # ë””ìŠ¤í¬ ìºì‹œ
        cache_file = os.path.join(self.cache_dir, 'parsed', f'{document_id}.pkl')
        with open(cache_file, 'wb') as f:
            pickle.dump(parsed_data, f)

        print(f"ğŸ’¾ íŒŒì‹± ê²°ê³¼ ìºì‹±: {document_id}")

    def invalidate(self, file_path: Optional[str] = None):
        """
        ìºì‹œ ë¬´íš¨í™”

        Args:
            file_path: íŠ¹ì • íŒŒì¼ë§Œ ë¬´íš¨í™” (Noneì´ë©´ ì „ì²´)
        """
        if file_path is None:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            self.memory_cache.clear()
            self.metadata.clear()
            print("ğŸ—‘ï¸  ì „ì²´ ìºì‹œ ì‚­ì œë¨")
        else:
            # íŠ¹ì • íŒŒì¼ ìºì‹œ ì‚­ì œ
            cache_key = self._get_cache_key(file_path)

            if cache_key in self.memory_cache:
                del self.memory_cache[cache_key]

            if cache_key in self.metadata:
                del self.metadata[cache_key]

            # ë””ìŠ¤í¬ ìºì‹œ íŒŒì¼ ì‚­ì œ
            cache_file = os.path.join(self.cache_dir, 'documents', f'{cache_key}.txt')
            if os.path.exists(cache_file):
                os.remove(cache_file)

            print(f"ğŸ—‘ï¸  ìºì‹œ ì‚­ì œ: {file_path}")

        self._save_metadata()

    def get_cache_stats(self) -> Dict:
        """
        ìºì‹œ í†µê³„ ì •ë³´

        Returns:
            {
                'memory_cache_size': int,
                'disk_cache_size': int,
                'total_cached_files': int,
                'total_cache_size_mb': float
            }
        """
        # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° (ëŒ€ëµì )
        memory_size = sum(
            len(str(v)) if isinstance(v, str) else len(str(v))
            for v in self.memory_cache.values()
        )

        # ë””ìŠ¤í¬ ìºì‹œ í¬ê¸°
        disk_size = 0
        for cache_type in ['documents', 'parsed']:
            cache_path = os.path.join(self.cache_dir, cache_type)
            if os.path.exists(cache_path):
                for file in os.listdir(cache_path):
                    file_path = os.path.join(cache_path, file)
                    if os.path.isfile(file_path):
                        disk_size += os.path.getsize(file_path)

        return {
            'memory_cache_items': len(self.memory_cache),
            'disk_cached_files': len(self.metadata),
            'memory_size_mb': memory_size / (1024 * 1024),
            'disk_size_mb': disk_size / (1024 * 1024),
            'total_size_mb': (memory_size + disk_size) / (1024 * 1024)
        }

    def print_stats(self):
        """ìºì‹œ í†µê³„ ì¶œë ¥"""
        stats = self.get_cache_stats()
        print("\nğŸ“Š ìºì‹œ í†µê³„")
        print(f"   ë©”ëª¨ë¦¬ ìºì‹œ: {stats['memory_cache_items']}ê°œ í•­ëª©")
        print(f"   ë””ìŠ¤í¬ ìºì‹œ: {stats['disk_cached_files']}ê°œ íŒŒì¼")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {stats['memory_size_mb']:.2f} MB")
        print(f"   ë””ìŠ¤í¬ ì‚¬ìš©: {stats['disk_size_mb']:.2f} MB")
        print(f"   ì´ ìºì‹œ í¬ê¸°: {stats['total_size_mb']:.2f} MB\n")

    def warm_up(self, file_patterns: list):
        """
        ìºì‹œ ì˜ˆì—´ (ìì£¼ ì‚¬ìš©ë˜ëŠ” íŒŒì¼ë“¤ì„ ë¯¸ë¦¬ ë¡œë“œ)

        Args:
            file_patterns: íŒŒì¼ íŒ¨í„´ ëª©ë¡ (ì˜ˆ: ['docs/requirements/**/*.md'])
        """
        from glob import glob

        print("ğŸ”¥ ìºì‹œ ì˜ˆì—´ ì¤‘...")
        loaded_count = 0

        for pattern in file_patterns:
            files = glob(pattern, recursive=True)
            for file_path in files:
                if os.path.isfile(file_path):
                    self.get_or_load(file_path)
                    loaded_count += 1

        print(f"âœ… {loaded_count}ê°œ íŒŒì¼ ìºì‹± ì™„ë£Œ\n")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    # ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    cache = CacheManager()

    # ìºì‹œ í†µê³„ ì¶œë ¥
    cache.print_stats()

    # ì˜ˆì‹œ: FR ë¬¸ì„œ ìºì‹±
    fr_file = 'docs/requirements/modules/inventory/FR-INV-001.md'
    if os.path.exists(fr_file):
        content = cache.get_or_load(fr_file)
        if content:
            print(f"FR ë¬¸ì„œ ê¸¸ì´: {len(content)} ë¬¸ì")

            # íŒŒì‹±ëœ ê²°ê³¼ ìºì‹± ì˜ˆì‹œ
            parsed_data = {
                'id': 'FR-INV-001',
                'title': 'ì¬ê³  ì¡°íšŒ ê¸°ëŠ¥',
                'requirements': ['...']
            }
            cache.set_parsed('FR-INV-001', parsed_data)

            # íŒŒì‹± ê²°ê³¼ ì¬ì‚¬ìš©
            cached_parsed = cache.get_parsed('FR-INV-001')
            print(f"íŒŒì‹± ê²°ê³¼: {cached_parsed['title']}")

    # ìºì‹œ ì˜ˆì—´ ì˜ˆì‹œ
    # cache.warm_up([
    #     'docs/requirements/modules/**/*.md',
    #     'docs/design/**/*.md'
    # ])

    # ìºì‹œ í†µê³„ ì¶œë ¥
    cache.print_stats()
