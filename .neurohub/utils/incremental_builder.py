"""
ì¦ë¶„ ë¹Œë“œ ì‹œìŠ¤í…œ (Incremental Build System)

íŒŒì¼ ë³€ê²½ ê°ì§€ ë° ì˜ì¡´ì„± ê¸°ë°˜ ì„ íƒì  ì¬ìƒì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
- íŒŒì¼ í•´ì‹±ì„ í†µí•œ ì‹¤ì œ ë³€ê²½ ê°ì§€
- FR â†’ Design â†’ Code â†’ Tests ì˜ì¡´ì„± ê·¸ë˜í”„
- ë³€ê²½ëœ ë¶€ë¶„ë§Œ ì¬ìƒì„±í•˜ì—¬ 10-20ë°° ì„±ëŠ¥ í–¥ìƒ

ì‚¬ìš© ì˜ˆì‹œ:
    builder = IncrementalBuilder()
    result = builder.generate_if_changed('docs/requirements/modules/inventory/FR-INV-001.md', 'inventory')
    if result['regenerated']:
        print(f"ì¬ìƒì„±ë¨: {result['artifacts']}")
    else:
        print("ë³€ê²½ ì—†ìŒ, ìºì‹œ ì‚¬ìš©")
"""

import hashlib
import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Set
from datetime import datetime
import networkx as nx


class IncrementalBuilder:
    """ì¦ë¶„ ë¹Œë“œ ì‹œìŠ¤í…œ - ë³€ê²½ëœ íŒŒì¼ë§Œ ì¬ìƒì„±"""

    def __init__(self, cache_file: str = '.neurohub/cache/build_cache.json'):
        """
        Args:
            cache_file: ë¹Œë“œ ìºì‹œ íŒŒì¼ ê²½ë¡œ
        """
        self.cache_file = cache_file
        self.cache = self._load_cache()
        self.dependency_graph = nx.DiGraph()
        self._build_dependency_graph()

    def _load_cache(self) -> Dict:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {'file_hashes': {}, 'artifacts': {}, 'metadata': {}}
        return {'file_hashes': {}, 'artifacts': {}, 'metadata': {}}

    def _save_cache(self):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, indent=2, ensure_ascii=False)

    def _hash_file(self, file_path: str) -> str:
        """íŒŒì¼ì˜ SHA-256 í•´ì‹œ ê³„ì‚°"""
        if not os.path.exists(file_path):
            return ''

        hasher = hashlib.sha256()
        with open(file_path, 'rb') as f:
            hasher.update(f.read())
        return hasher.hexdigest()

    def _build_dependency_graph(self):
        """
        ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•
        FR â†’ Design Docs â†’ Code Files â†’ Test Files
        """
        # ì˜ˆì‹œ ì˜ì¡´ì„± (ì‹¤ì œë¡œëŠ” íŒŒì¼ ë¶„ì„ìœ¼ë¡œ ë™ì  ìƒì„±)
        # ì´ ë©”ì„œë“œëŠ” ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì˜ì¡´ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤
        pass

    def has_file_changed(self, file_path: str) -> bool:
        """
        íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸

        Args:
            file_path: í™•ì¸í•  íŒŒì¼ ê²½ë¡œ

        Returns:
            True if íŒŒì¼ì´ ë³€ê²½ë¨, False otherwise
        """
        current_hash = self._hash_file(file_path)
        cached_hash = self.cache['file_hashes'].get(file_path, '')

        return current_hash != cached_hash

    def get_affected_files(self, changed_file: str) -> Set[str]:
        """
        ë³€ê²½ëœ íŒŒì¼ì— ì˜í•´ ì˜í–¥ë°›ëŠ” ëª¨ë“  íŒŒì¼ ì°¾ê¸°

        Args:
            changed_file: ë³€ê²½ëœ íŒŒì¼ ê²½ë¡œ

        Returns:
            ì˜í–¥ë°›ëŠ” íŒŒì¼ ê²½ë¡œ ì§‘í•©
        """
        affected = {changed_file}

        # ì˜ì¡´ì„± ê·¸ë˜í”„ì—ì„œ downstream íŒŒì¼ë“¤ ì°¾ê¸°
        if self.dependency_graph.has_node(changed_file):
            descendants = nx.descendants(self.dependency_graph, changed_file)
            affected.update(descendants)

        return affected

    def mark_regenerated(self, file_path: str, artifacts: List[str]):
        """
        íŒŒì¼ì´ ì¬ìƒì„±ë˜ì—ˆìŒì„ ê¸°ë¡

        Args:
            file_path: ì¬ìƒì„±ì˜ ì›ì¸ì´ ëœ íŒŒì¼
            artifacts: ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸ ëª©ë¡
        """
        current_hash = self._hash_file(file_path)
        self.cache['file_hashes'][file_path] = current_hash
        self.cache['artifacts'][file_path] = {
            'files': artifacts,
            'timestamp': datetime.now().isoformat()
        }
        self.cache['metadata'][file_path] = {
            'last_build': datetime.now().isoformat(),
            'hash': current_hash
        }
        self._save_cache()

    def generate_if_changed(self, source_file: str, module: str,
                           agent_executor=None) -> Dict:
        """
        íŒŒì¼ì´ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ ì¬ìƒì„±

        Args:
            source_file: ì†ŒìŠ¤ íŒŒì¼ (FR ë¬¸ì„œ ë“±)
            module: ëª¨ë“ˆ ì´ë¦„
            agent_executor: ì—ì´ì „íŠ¸ ì‹¤í–‰ í•¨ìˆ˜ (ì„ íƒì )

        Returns:
            {
                'regenerated': bool,  # ì¬ìƒì„± ì—¬ë¶€
                'reason': str,  # ì¬ìƒì„± ì´ìœ  ë˜ëŠ” ìŠ¤í‚µ ì´ìœ 
                'artifacts': List[str],  # ìƒì„±ëœ íŒŒì¼ ëª©ë¡
                'cached_from': str,  # ìºì‹œ ì‚¬ìš© ì‹œ ì›ë³¸ íŒŒì¼
                'time_saved': float  # ì ˆì•½ëœ ì‹œê°„ (ì´ˆ)
            }
        """
        # 1. íŒŒì¼ ë³€ê²½ í™•ì¸
        if not self.has_file_changed(source_file):
            cached_artifacts = self.cache['artifacts'].get(source_file, {})
            return {
                'regenerated': False,
                'reason': 'no_changes_detected',
                'artifacts': cached_artifacts.get('files', []),
                'cached_from': source_file,
                'time_saved': self._estimate_time_saved(source_file)
            }

        # 2. ë³€ê²½ëœ ê²½ìš°, ì˜í–¥ë°›ëŠ” íŒŒì¼ë“¤ ì°¾ê¸°
        affected_files = self.get_affected_files(source_file)

        # 3. ì¬ìƒì„± ì‹¤í–‰ (agent_executorê°€ ì œê³µëœ ê²½ìš°)
        generated_artifacts = []
        if agent_executor:
            print(f"ğŸ”„ ì¬ìƒì„± ì¤‘: {source_file}")
            print(f"   ì˜í–¥ë°›ëŠ” íŒŒì¼: {len(affected_files)}ê°œ")

            try:
                result = agent_executor(source_file, module, affected_files)
                generated_artifacts = result.get('artifacts', [])
            except Exception as e:
                print(f"âŒ ì¬ìƒì„± ì‹¤íŒ¨: {e}")
                return {
                    'regenerated': False,
                    'reason': f'generation_failed: {str(e)}',
                    'artifacts': [],
                    'error': str(e)
                }

        # 4. ìºì‹œ ì—…ë°ì´íŠ¸
        self.mark_regenerated(source_file, generated_artifacts)

        return {
            'regenerated': True,
            'reason': 'file_changed',
            'artifacts': generated_artifacts,
            'affected_files': list(affected_files),
            'source_hash': self._hash_file(source_file)
        }

    def _estimate_time_saved(self, file_path: str) -> float:
        """
        ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì ˆì•½ëœ ì‹œê°„ ì¶”ì •

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            ì ˆì•½ëœ ì‹œê°„ (ì´ˆ)
        """
        # FR ë¬¸ì„œ íƒ€ì…ì— ë”°ë¼ ì¶”ì • ì‹œê°„ ë°˜í™˜
        if 'FR-' in file_path:
            # FR ì¬ìƒì„± ì‹œ í‰ê·  5-15ë¶„ ì†Œìš”
            return 600  # 10ë¶„
        elif 'AC-' in file_path:
            return 300  # 5ë¶„
        elif 'API-' in file_path or 'DB-' in file_path:
            return 180  # 3ë¶„
        return 0

    def invalidate_cache(self, file_pattern: Optional[str] = None):
        """
        ìºì‹œ ë¬´íš¨í™”

        Args:
            file_pattern: íŠ¹ì • íŒ¨í„´ì˜ íŒŒì¼ë§Œ ë¬´íš¨í™” (Noneì´ë©´ ì „ì²´)
        """
        if file_pattern is None:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            self.cache = {'file_hashes': {}, 'artifacts': {}, 'metadata': {}}
            print("ğŸ—‘ï¸  ì „ì²´ ìºì‹œ ì‚­ì œë¨")
        else:
            # íŒ¨í„´ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ë§Œ ì‚­ì œ
            keys_to_remove = [k for k in self.cache['file_hashes'].keys()
                            if file_pattern in k]
            for key in keys_to_remove:
                self.cache['file_hashes'].pop(key, None)
                self.cache['artifacts'].pop(key, None)
                self.cache['metadata'].pop(key, None)
            print(f"ğŸ—‘ï¸  {len(keys_to_remove)}ê°œ ìºì‹œ í•­ëª© ì‚­ì œë¨")

        self._save_cache()

    def get_cache_stats(self) -> Dict:
        """
        ìºì‹œ í†µê³„ ì •ë³´ ë°˜í™˜

        Returns:
            {
                'total_cached_files': int,
                'total_artifacts': int,
                'total_time_saved': float,
                'cache_size_mb': float
            }
        """
        total_artifacts = sum(len(v.get('files', []))
                            for v in self.cache['artifacts'].values())

        total_time_saved = sum(self._estimate_time_saved(k)
                              for k in self.cache['file_hashes'].keys())

        cache_size_bytes = 0
        if os.path.exists(self.cache_file):
            cache_size_bytes = os.path.getsize(self.cache_file)

        return {
            'total_cached_files': len(self.cache['file_hashes']),
            'total_artifacts': total_artifacts,
            'total_time_saved_seconds': total_time_saved,
            'total_time_saved_minutes': total_time_saved / 60,
            'cache_size_mb': cache_size_bytes / (1024 * 1024)
        }

    def print_cache_stats(self):
        """ìºì‹œ í†µê³„ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        stats = self.get_cache_stats()
        print("\nğŸ“Š ì¦ë¶„ ë¹Œë“œ ìºì‹œ í†µê³„")
        print(f"   ìºì‹œëœ íŒŒì¼: {stats['total_cached_files']}ê°œ")
        print(f"   ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸: {stats['total_artifacts']}ê°œ")
        print(f"   ì ˆì•½ëœ ì‹œê°„: {stats['total_time_saved_minutes']:.1f}ë¶„")
        print(f"   ìºì‹œ í¬ê¸°: {stats['cache_size_mb']:.2f} MB\n")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    # ì¦ë¶„ ë¹Œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    builder = IncrementalBuilder()

    # ìºì‹œ í†µê³„ ì¶œë ¥
    builder.print_cache_stats()

    # ì˜ˆì‹œ: FR ë¬¸ì„œ ë³€ê²½ í™•ì¸
    fr_file = 'docs/requirements/modules/inventory/FR-INV-001.md'
    if os.path.exists(fr_file):
        result = builder.generate_if_changed(fr_file, 'inventory')

        if result['regenerated']:
            print(f"âœ… {fr_file} ì¬ìƒì„±ë¨")
            print(f"   ìƒì„±ëœ íŒŒì¼: {len(result['artifacts'])}ê°œ")
        else:
            print(f"â­ï¸  {fr_file} ë³€ê²½ ì—†ìŒ (ìºì‹œ ì‚¬ìš©)")
            print(f"   ì ˆì•½ëœ ì‹œê°„: {result['time_saved']/60:.1f}ë¶„")

    # ìºì‹œ ë¬´íš¨í™” ì˜ˆì‹œ
    # builder.invalidate_cache('inventory')  # inventory ê´€ë ¨ ìºì‹œë§Œ ì‚­ì œ
    # builder.invalidate_cache()  # ì „ì²´ ìºì‹œ ì‚­ì œ
